{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as the RNN.ipynb except the images are cropped to only include the subjects face\n",
    "from matplotlib import pyplot as plt\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from matplotlib.patches import Rectangle\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from scipy.spatial.distance import cosine\n",
    "from keras.utils.layer_utils import get_source_inputs\n",
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_W = 224\n",
    "IMG_H = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "\n",
    "MAX_SEQ_LENGTH = 100\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"../Data/mirror-data2.csv\")\n",
    "df1 = df1[df1.Action != (\"Talking\" or \"talking\")]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face_from_image(frame, required_size=(224, 224)):\n",
    "    # load image and detect faces\n",
    "    image = frame\n",
    "    detector = MTCNN()\n",
    "    faces = detector.detect_faces(image)\n",
    "  \n",
    "    # extract the bounding box from the requested face\n",
    "    x1, y1, width, height = faces[0]['box']\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "\n",
    "    # extract the face\n",
    "    face_boundary = image[y1:y2, x1:x2]\n",
    "\n",
    "    # resize pixels to the model size\n",
    "    face_image = Image.fromarray(face_boundary)\n",
    "    face_image = face_image.resize(required_size)\n",
    "    face_array = np.asarray(face_image)\n",
    "    \n",
    "\n",
    "    return face_array\n",
    "\n",
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(frames,  resize=(IMG_H, IMG_W)):\n",
    "    \n",
    "    frames = []\n",
    "\n",
    "    \n",
    "    \n",
    "    for frame in frames:\n",
    "\n",
    "        frame = extract_face_from_image(frame)\n",
    "        frame = cv2.resize(frame, resize)\n",
    "        frame = frame[:, :, [2, 1, 0]]\n",
    "        frames.append(frame)\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    return np.array(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_H, IMG_W, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_H, IMG_W, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_processor = keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=np.unique(df1['Action'])\n",
    ")\n",
    "print(label_processor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "dfTrain = pd.DataFrame()\n",
    "dfTest = pd.DataFrame()\n",
    "\n",
    "while i<len(df1):\n",
    "    if i%5==0:\n",
    "        dfTest = dfTest.append(df1.iloc[[i]])\n",
    "    else :\n",
    "        dfTrain = dfTrain.append(df1.iloc[[i]])\n",
    "\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video-name\"].values.tolist()\n",
    "    start_nums = df[\"yawn-start\"].values.tolist()\n",
    "    # print(video_paths)\n",
    "    labels = df[\"Action\"].values\n",
    "    labels = label_processor(labels[..., None]).numpy()\n",
    "\n",
    "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
    "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
    "    # masked with padding or not.\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        videoName = path\n",
    "        number = start_nums[idx]\n",
    "        frames = []\n",
    "\n",
    "        while(len(frames)<=MAX_SEQ_LENGTH):\n",
    "            path = \"../Data/VideoFrames/\"+videoName+\"/\"+videoName+\"_\"+f\"{number:03}\"+\".jpg\"\n",
    "            frames.append(image.load_img(path, target_size=(224, 224, 3)))\n",
    "            number+=1\n",
    "        print(path)\n",
    "\n",
    "\n",
    "        frames = load_video(frames)\n",
    "        frames = frames[None, ...]\n",
    "        # Initialize placeholders to store the masks and features of the current video.\n",
    "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "        # Extract features from the frames of the current video.\n",
    "        \n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :]\n",
    "                )\n",
    "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "        \n",
    "\n",
    "    return (frame_features, frame_masks), labels\n",
    "\n",
    "\n",
    "train_data, train_labels = prepare_all_videos(dfTrain, \"../Data/Mirror\")\n",
    "test_data, test_labels = prepare_all_videos(dfTest, \"../Data/Mirror\")\n",
    "\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility for our sequence model.\n",
    "def get_sequence_model():\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "    x = keras.layers.GRU(16, return_sequences=True)(\n",
    "        frame_features_input, mask=mask_input\n",
    "    )\n",
    "    x = keras.layers.GRU(8)(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Adam(epsilon=1, learning_rate=5), metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return rnn_model\n",
    "\n",
    "\n",
    "# Utility for running experiments.\n",
    "def run_experiment():\n",
    "    filepath = \"./video_classifier\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1]],\n",
    "        train_labels,\n",
    "        validation_split=0.3,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "\n",
    "    seq_model.load_weights(filepath)\n",
    "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history, seq_model\n",
    "\n",
    "\n",
    "_, sequence_model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = plt.imread('../Data/VideoFrames/17-FemaleSunGlasses-Normal.avi/17-FemaleSunGlasses-Normal.avi_000.jpg')\n",
    "detector = MTCNN()\n",
    "\n",
    "faces = detector.detect_faces(image)\n",
    "for face in faces:\n",
    "  print(face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_faces(image_path, faces):\n",
    "  # display image\n",
    "  image = plt.imread(image_path)\n",
    "  plt.imshow(image)\n",
    "\n",
    "  ax = plt.gca()\n",
    "\n",
    "  # for each face, draw a rectangle based on coordinates\n",
    "  for face in faces:\n",
    "    x, y, width, height = face['box']\n",
    "    face_border = Rectangle((x, y), width, height,\n",
    "                          fill=False, color='red')\n",
    "    ax.add_patch(face_border)\n",
    "  plt.show()\n",
    "\n",
    "highlight_faces('../Data/VideoFrames/17-FemaleSunGlasses-Normal.avi/17-FemaleSunGlasses-Normal.avi_000.jpg', faces)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face_from_image(image_path, required_size=(224, 224)):\n",
    "  # load image and detect faces\n",
    "  image = plt.imread(image_path)\n",
    "  detector = MTCNN()\n",
    "  faces = detector.detect_faces(image)\n",
    "\n",
    "  face_images = []\n",
    "\n",
    "  for face in faces:\n",
    "    # extract the bounding box from the requested face\n",
    "    x1, y1, width, height = face['box']\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "\n",
    "    # extract the face\n",
    "    face_boundary = image[y1:y2, x1:x2]\n",
    "\n",
    "    # resize pixels to the model size\n",
    "    face_image = Image.fromarray(face_boundary)\n",
    "    face_image = face_image.resize(required_size)\n",
    "    face_array = np.asarray(face_image)\n",
    "    face_images.append(face_array)\n",
    "\n",
    "  return face_images\n",
    "\n",
    "extracted_face = extract_face_from_image('../Data/VideoFrames/17-FemaleSunGlasses-Normal.avi/17-FemaleSunGlasses-Normal.avi_000.jpg')\n",
    "\n",
    "# Display the first face from the extracted faces\n",
    "plt.imshow(extracted_face[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_scores(faces):\n",
    "  samples = asarray(faces, 'float32')\n",
    "\n",
    "  # prepare the data for the model\n",
    "  samples = preprocess_input(samples, version=2)\n",
    "\n",
    "  # create a vggface model object\n",
    "  model = VGGFace(model='resnet50',\n",
    "      include_top=False,\n",
    "      input_shape=(224, 224, 3),\n",
    "      pooling='avg')\n",
    "\n",
    "  # perform prediction\n",
    "  return model.predict(samples)\n",
    "\n",
    "faces = [extract_face_from_image(image_path)\n",
    "         for image_path in ['iacocca_1.jpg', 'iacocca_2.jpg']]\n",
    "\n",
    "model_scores = get_model_scores(faces)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3deb5058a37522a2e4ac59617ed5a8c2cf9a3176becbc0158883759547460f28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
