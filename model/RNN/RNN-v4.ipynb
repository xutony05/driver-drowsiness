{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new RNN model but keeping the features and masking the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow_docs.vis import embed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_W = 224\n",
    "IMG_H = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"../Data/mirror-data2.csv\")\n",
    "df1 = df1[df1.Action != (\"Talking\" or \"talking\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(frames,  resize=(IMG_H, IMG_W)):\n",
    "    \n",
    "    frames = []\n",
    "    for frame in frames:\n",
    "\n",
    "        frame = crop_center_square(frame)\n",
    "        frame = cv2.resize(frame, resize)\n",
    "        frame = frame[:, :, [2, 1, 0]]\n",
    "        frames.append(frame)\n",
    "\n",
    "        \n",
    "    \n",
    "    return np.array(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.VGG16(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_H, IMG_W, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_H, IMG_W, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Normal', 'Talking&Yawning', 'Yawning', 'talking']\n"
     ]
    }
   ],
   "source": [
    "label_processor = keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=np.unique(df1['Action'])\n",
    ")\n",
    "print(label_processor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "dfTrain = pd.DataFrame()\n",
    "dfTest = pd.DataFrame()\n",
    "\n",
    "while i<len(df1):\n",
    "    if i%5==0:\n",
    "        dfTest = dfTest.append(df1.iloc[[i]])\n",
    "    else :\n",
    "        dfTrain = dfTrain.append(df1.iloc[[i]])\n",
    "\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/VideoFrames/1-FemaleNoGlasses-Yawning.avi/1-FemaleNoGlasses-Yawning.avi_070.jpg\n",
      "../Data/VideoFrames/2-FemaleNoGlasses-Normal.avi/2-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/2-FemaleNoGlasses-Yawning.avi/2-FemaleNoGlasses-Yawning.avi_395.jpg\n",
      "../Data/VideoFrames/3-FemaleGlasses-Normal.avi/3-FemaleGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/4-FemaleGlasses-Normal.avi/4-FemaleGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/4-FemaleGlasses-Yawning.avi/4-FemaleGlasses-Yawning.avi_030.jpg\n",
      "../Data/VideoFrames/5-FemaleGlasses-Normal.avi/5-FemaleGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/5-FemaleGlasses-Yawning.avi/5-FemaleGlasses-Yawning.avi_160.jpg\n",
      "../Data/VideoFrames/6-FemaleNoGlasses-Yawning.avi/6-FemaleNoGlasses-Yawning.avi_140.jpg\n",
      "../Data/VideoFrames/7-FemaleGlasses-Normal.avi/7-FemaleGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/7-FemaleGlasses-Yawning.avi/7-FemaleGlasses-Yawning.avi_105.jpg\n",
      "../Data/VideoFrames/8-FemaleGlasses-Normal.avi/8-FemaleGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/9-FemaleNoGlasses-Normal.avi/9-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/9-FemaleNoGlasses-Yawning.avi/9-FemaleNoGlasses-Yawning.avi_065.jpg\n",
      "../Data/VideoFrames/10-FemaleNoGlasses-Normal.avi/10-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/10-FemaleNoGlasses-Yawning.avi/10-FemaleNoGlasses-Yawning.avi_040.jpg\n",
      "../Data/VideoFrames/11-FemaleNoGlasses-Yawning.avi/11-FemaleNoGlasses-Yawning.avi_180.jpg\n",
      "../Data/VideoFrames/12-FemaleNoGlasses-Normal.avi/12-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/12-FemaleNoGlasses-Yawning.avi/12-FemaleNoGlasses-Yawning.avi_565.jpg\n",
      "../Data/VideoFrames/13-FemaleNoGlasses-Normal.avi/13-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/14-FemaleNoGlasses-Normal.avi/14-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/14-FemaleNoGlasses-Yawning.avi/14-FemaleNoGlasses-Yawning.avi_250.jpg\n",
      "../Data/VideoFrames/15-FemaleGlasses-Normal.avi/15-FemaleGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/15-FemaleGlasses-Yawning.avi/15-FemaleGlasses-Yawning.avi_160.jpg\n",
      "../Data/VideoFrames/15-FemaleSunGlasses-Yawning.avi/15-FemaleSunGlasses-Yawning.avi_160.jpg\n",
      "../Data/VideoFrames/16-FemaleGlasses-Normal.avi/16-FemaleGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/16-FemaleGlasses-Yawning.avi/16-FemaleGlasses-Yawning.avi_080.jpg\n",
      "../Data/VideoFrames/17-FemaleNoGlasses-Normal.avi/17-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/17-FemaleSunGlasses-Normal.avi/17-FemaleSunGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/17-FemaleSunGlasses-Yawning.avi/17-FemaleSunGlasses-Yawning.avi_020.jpg\n",
      "../Data/VideoFrames/18-FemaleNoGlasses-Normal.avi/18-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/18-FemaleNoGlasses-Yawning.avi/18-FemaleNoGlasses-Yawning.avi_090.jpg\n",
      "../Data/VideoFrames/18-FemaleSunGlasses-Yawning.avi/18-FemaleSunGlasses-Yawning.avi_200.jpg\n",
      "../Data/VideoFrames/19-FemaleNoGlasses-Normal.avi/19-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/19-FemaleNoGlasses-Yawning.avi/19-FemaleNoGlasses-Yawning.avi_160.jpg\n",
      "../Data/VideoFrames/20-FemaleNoGlasses-Normal.avi/20-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/21-FemaleNoGlasses-Normal.avi/21-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/21-FemaleNoGlasses-Yawning.avi/21-FemaleNoGlasses-Yawning.avi_120.jpg\n",
      "../Data/VideoFrames/22-FemaleNoGlasses-Normal.avi/22-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/22-FemaleNoGlasses-Yawning.avi/22-FemaleNoGlasses-Yawning.avi_220.jpg\n",
      "../Data/VideoFrames/22-FemaleSunGlasses-Yawning.avi/22-FemaleSunGlasses-Yawning.avi_130.jpg\n",
      "../Data/VideoFrames/23-FemaleNoGlasses-Normal.avi/23-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/23-FemaleNoGlasses-Talking&Yawning.avi/23-FemaleNoGlasses-Talking&Yawning.avi_020.jpg\n",
      "../Data/VideoFrames/23-FemaleNoGlasses-Yawning.avi/23-FemaleNoGlasses-Yawning.avi_164.jpg\n",
      "../Data/VideoFrames/24-FemaleNoGlasses-Yawning.avi/24-FemaleNoGlasses-Yawning.avi_190.jpg\n",
      "../Data/VideoFrames/25-FemaleNoGlasses-Normal.avi/25-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/25-FemaleNoGlasses-Yawning.avi/25-FemaleNoGlasses-Yawning.avi_060.jpg\n",
      "../Data/VideoFrames/25-FemaleSunGlasses-Normal.avi/25-FemaleSunGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/26-FemaleGlasses-Normal.avi/26-FemaleGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/26-FemaleGlasses-Yawning.avi/26-FemaleGlasses-Yawning.avi_160.jpg\n",
      "../Data/VideoFrames/26-FemaleSunGlasses-Normal.avi/26-FemaleSunGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/26-FemaleSunGlasses-Yawning.avi/26-FemaleSunGlasses-Yawning.avi_045.jpg\n",
      "../Data/VideoFrames/27-FemaleNoGlasses-Talking&Yawning.avi/27-FemaleNoGlasses-Talking&Yawning.avi_020.jpg\n",
      "../Data/VideoFrames/27-FemaleSunGlasses-Normal.avi/27-FemaleSunGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/27-FemaleSunGlasses-Yawning.avi/27-FemaleSunGlasses-Yawning.avi_180.jpg\n",
      "../Data/VideoFrames/28-FemaleNoGlasses-Normal.avi/28-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/29-FemaleNoGlasses-Normal.avi/29-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/29-FemaleNoGlasses-Yawning.avi/29-FemaleNoGlasses-Yawning.avi_170.jpg\n",
      "../Data/VideoFrames/30-FemaleNoGlasses-Normal.avi/30-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/30-FemaleNoGlasses-Yawning.avi/30-FemaleNoGlasses-Yawning.avi_370.jpg\n",
      "../Data/VideoFrames/31-FemaleGlasses-Yawning.avi/31-FemaleGlasses-Yawning.avi_210.jpg\n",
      "../Data/VideoFrames/31-FemaleNoGlasses-Normal.avi/31-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/31-FemaleNoGlasses-Yawning.avi/31-FemaleNoGlasses-Yawning.avi_198.jpg\n",
      "../Data/VideoFrames/32-FemaleSunGlasses-Normal.avi/32-FemaleSunGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/33-FemaleNoGlasses-Normal.avi/33-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/33-FemaleNoGlasses-Yawning.avi/33-FemaleNoGlasses-Yawning.avi_265.jpg\n",
      "../Data/VideoFrames/34-FemaleNoGlasses-Normal.avi/34-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/34-FemaleNoGlasses-Talking&Yawning.avi/34-FemaleNoGlasses-Talking&Yawning.avi_020.jpg\n",
      "../Data/VideoFrames/35-FemaleNoGlasses-Normal.avi/35-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/35-FemaleNoGlasses-Talking&Yawning.avi/35-FemaleNoGlasses-Talking&Yawning.avi_020.jpg\n",
      "../Data/VideoFrames/35-FemaleNoGlasses-Yawning.avi/35-FemaleNoGlasses-Yawning.avi_058.jpg\n",
      "../Data/VideoFrames/36-FemaleNoGlasses-Normal.avi/36-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/36-FemaleNoGlasses-Yawning.avi/36-FemaleNoGlasses-Yawning.avi_215.jpg\n",
      "../Data/VideoFrames/37-FemaleNoGlasses-Normal.avi/37-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/37-FemaleNoGlasses-Talking&Yawning.avi/37-FemaleNoGlasses-Talking&Yawning.avi_020.jpg\n",
      "../Data/VideoFrames/37-FemaleNoGlasses-Yawning.avi/37-FemaleNoGlasses-Yawning.avi_095.jpg\n",
      "../Data/VideoFrames/38-FemaleNoGlasses-Yawning.avi/38-FemaleNoGlasses-Yawning.avi_180.jpg\n",
      "../Data/VideoFrames/39-FemaleNoGlasses-Normal.avi/39-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/39-FemaleNoGlasses-Talking&Yawning.avi/39-FemaleNoGlasses-Talking&Yawning.avi_020.jpg\n",
      "../Data/VideoFrames/39-FemaleNoGlasses-Yawning.avi/39-FemaleNoGlasses-Yawning.avi_140.jpg\n",
      "../Data/VideoFrames/40-FemaleNoGlasses-Yawning.avi/40-FemaleNoGlasses-Yawning.avi_306.jpg\n",
      "../Data/VideoFrames/41-FemaleGlasses-Normal.avi/41-FemaleGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/41-FemaleGlasses-Yawning.avi/41-FemaleGlasses-Yawning.avi_210.jpg\n",
      "../Data/VideoFrames/42-FemaleSunGlasses-Normal.avi/42-FemaleSunGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/43-FemaleNoGlasses-Normal.avi/43-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/43-FemaleNoGlasses-Yawning.avi/43-FemaleNoGlasses-Yawning.avi_210.jpg\n",
      "../Data/VideoFrames/1-MaleNoGlasses-Yawning.avi/1-MaleNoGlasses-Yawning.avi_190.jpg\n",
      "../Data/VideoFrames/1-MaleSunGlasses-Normal.avi/1-MaleSunGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/2-MaleGlasses-Normal.avi/2-MaleGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/2-MaleGlasses-Yawning.avi/2-MaleGlasses-Yawning.avi_150.jpg\n",
      "../Data/VideoFrames/3-MaleGlasses-Normal.avi/3-MaleGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/3-MaleGlasses-Yawning.avi/3-MaleGlasses-Yawning.avi_095.jpg\n",
      "../Data/VideoFrames/3-MaleNoGlasses-Yawning.avi/3-MaleNoGlasses-Yawning.avi_110.jpg\n",
      "../Data/VideoFrames/4-MaleNoGlasses-Normal.avi/4-MaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/4-MaleNoGlasses-Yawning.avi/4-MaleNoGlasses-Yawning.avi_075.jpg\n",
      "../Data/VideoFrames/5-MaleNoGlasses-Normal.avi/5-MaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/6-MaleNoGlasses-Normal.avi/6-MaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/6-MaleNoGlasses-Yawning.avi/6-MaleNoGlasses-Yawning.avi_047.jpg\n",
      "../Data/VideoFrames/7-MaleGlasses-Normal.avi/7-MaleGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/7-MaleGlasses-Yawning.avi/7-MaleGlasses-Yawning.avi_110.jpg\n",
      "../Data/VideoFrames/8-MaleGlassesBeard-Yawning.avi/8-MaleGlassesBeard-Yawning.avi_095.jpg\n",
      "../Data/VideoFrames/9-MaleNoGlasses-Normal.avi/9-MaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/9-MaleNoGlasses-Yawning.avi/9-MaleNoGlasses-Yawning.avi_100.jpg\n",
      "../Data/VideoFrames/10-MaleNoGlasses-Normal.avi/10-MaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/11-MaleGlasses-Normal.avi/11-MaleGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/11-MaleGlasses-Yawning.avi/11-MaleGlasses-Yawning.avi_340.jpg\n",
      "../Data/VideoFrames/12-MaleGlasses-Normal.avi/12-MaleGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/12-MaleGlasses-Yawning.avi/12-MaleGlasses-Yawning.avi_210.jpg\n",
      "../Data/VideoFrames/13-MaleGlasses-Yawning.avi/13-MaleGlasses-Yawning.avi_080.jpg\n",
      "../Data/VideoFrames/14-MaleGlasses-Normal.avi/14-MaleGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/15-MaleNoGlasses-Normal.avi/15-MaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/15-MaleNoGlasses-Talking.avi/15-MaleNoGlasses-Talking.avi_020.jpg\n",
      "../Data/VideoFrames/16-MaleNoGlasses-Normal.avi/16-MaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/17-MaleNoGlasses-Normal.avi/17-MaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/17-MaleNoGlasses-Yawning.avi/17-MaleNoGlasses-Yawning.avi_180.jpg\n",
      "../Data/VideoFrames/18-MaleNoGlasses-Normal.avi/18-MaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/19-MaleGlassesmoustache-Normal.avi/19-MaleGlassesmoustache-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/19-MaleGlassesmoustache-Yawning.avi/19-MaleGlassesmoustache-Yawning.avi_087.jpg\n",
      "../Data/VideoFrames/20-MaleGlasses-Normal.avi/20-MaleGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/20-MaleGlasses-Talking.avi/20-MaleGlasses-Talking.avi_020.jpg\n",
      "../Data/VideoFrames/21-MaleGlasses-Yawning.avi/21-MaleGlasses-Yawning.avi_166.jpg\n",
      "../Data/VideoFrames/22-MaleGlassesmoustache-Normal.avi/22-MaleGlassesmoustache-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/22-MaleGlassesmoustache-Yawning.avi/22-MaleGlassesmoustache-Yawning.avi_102.jpg\n",
      "../Data/VideoFrames/23-MaleGlassesBeard-Normal.avi/23-MaleGlassesBeard-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/23-MaleGlassesBeard-Yawning.avi/23-MaleGlassesBeard-Yawning.avi_190.jpg\n",
      "../Data/VideoFrames/23-MaleNoGlasses-Normal.avi/23-MaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/23-MaleNoGlasses-Yawning.avi/23-MaleNoGlasses-Yawning.avi_162.jpg\n",
      "../Data/VideoFrames/24-MaleGlasses-Normal.avi/24-MaleGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/25-MaleGlasses-Normal.avi/25-MaleGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/25-MaleGlasses-Yawning.avi/25-MaleGlasses-Yawning.avi_210.jpg\n",
      "../Data/VideoFrames/25-MaleSunGlasses-Normal.avi/25-MaleSunGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/25-MaleSunGlasses-Yawning.avi/25-MaleSunGlasses-Yawning.avi_090.jpg\n",
      "../Data/VideoFrames/26-MaleNoGlasses-Yawning.avi/26-MaleNoGlasses-Yawning.avi_140.jpg\n",
      "../Data/VideoFrames/27-MaleGlasses-Normal.avi/27-MaleGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/27-MaleGlasses-Yawning.avi/27-MaleGlasses-Yawning.avi_035.jpg\n",
      "../Data/VideoFrames/27-MaleNoGlasses-Normal.avi/27-MaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/28-MaleGlasses-Normal.avi/28-MaleGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/28-MaleGlasses-Yawning.avi/28-MaleGlasses-Yawning.avi_185.jpg\n",
      "../Data/VideoFrames/28-MaleNoGlasses-Normal.avi/28-MaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/28-MaleNoGlasses-Yawning.avi/28-MaleNoGlasses-Yawning.avi_178.jpg\n",
      "../Data/VideoFrames/29-MaleNoGlasses-Yawning.avi/29-MaleNoGlasses-Yawning.avi_165.jpg\n",
      "../Data/VideoFrames/30-MaleGlasses-Normal.avi/30-MaleGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/30-MaleGlasses-Talking&Yawning.avi/30-MaleGlasses-Talking&Yawning.avi_020.jpg\n",
      "../Data/VideoFrames/30-MaleGlasses-Yawning.avi/30-MaleGlasses-Yawning.avi_148.jpg\n",
      "../Data/VideoFrames/31-MaleGlasses-Yawning.avi/31-MaleGlasses-Yawning.avi_035.jpg\n",
      "../Data/VideoFrames/32-MaleGlasses-Normal.avi/32-MaleGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/32-MaleGlasses-Talking&Yawning.avi/32-MaleGlasses-Talking&Yawning.avi_020.jpg\n",
      "../Data/VideoFrames/32-MaleGlasses-Yawning.avi/32-MaleGlasses-Yawning.avi_230.jpg\n",
      "../Data/VideoFrames/33-MaleGlasses-Talking&Yawning.avi/33-MaleGlasses-Talking&Yawning.avi_020.jpg\n",
      "../Data/VideoFrames/33-MaleGlasses-Yawning.avi/33-MaleGlasses-Yawning.avi_180.jpg\n",
      "../Data/VideoFrames/34-MaleNoGlasses-Normal.avi/34-MaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/34-MaleNoGlasses-Yawning.avi/34-MaleNoGlasses-Yawning.avi_020.jpg\n",
      "../Data/VideoFrames/35-MaleNoGlasses-Yawning.avi/35-MaleNoGlasses-Yawning.avi_195.jpg\n",
      "../Data/VideoFrames/36-MaleNoGlasses-Normal.avi/36-MaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/36-MaleNoGlasses-Yawning.avi/36-MaleNoGlasses-Yawning.avi_160.jpg\n",
      "../Data/VideoFrames/36-MaleSunGlasses-Normal.avi/36-MaleSunGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/37-MaleNoGlasses-Normal.avi/37-MaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/37-MaleNoGlasses-Yawning.avi/37-MaleNoGlasses-Yawning.avi_190.jpg\n",
      "../Data/VideoFrames/38-MaleNoGlasses-Normal.avi/38-MaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/38-MaleNoGlasses-Yawning.avi/38-MaleNoGlasses-Yawning.avi_194.jpg\n",
      "../Data/VideoFrames/38-MaleSunGlasses-Yawning.avi/38-MaleSunGlasses-Yawning.avi_128.jpg\n",
      "../Data/VideoFrames/39-MaleGlasses-Normal.avi/39-MaleGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/39-MaleGlasses-Yawning.avi/39-MaleGlasses-Yawning.avi_390.jpg\n",
      "../Data/VideoFrames/40-MaleNoGlasses-Normal.avi/40-MaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/41-MaleNoGlasses-Normal.avi/41-MaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/41-MaleNoGlasses-Yawning.avi/41-MaleNoGlasses-Yawning.avi_165.jpg\n",
      "../Data/VideoFrames/42-MaleNoGlasses-Normal.avi/42-MaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/42-MaleNoGlasses-Yawning.avi/42-MaleNoGlasses-Yawning.avi_240.jpg\n",
      "../Data/VideoFrames/43-MaleNoGlasses-Yawning.avi/43-MaleNoGlasses-Yawning.avi_196.jpg\n",
      "../Data/VideoFrames/44-MaleNoGlasses-Normal.avi/44-MaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/44-MaleNoGlasses-Yawning.avi/44-MaleNoGlasses-Yawning.avi_090.jpg\n",
      "../Data/VideoFrames/45-MaleNoGlasses-Normal.avi/45-MaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/46-MaleGlasses-Normal.avi/46-MaleGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/46-MaleGlasses-Yawning.avi/46-MaleGlasses-Yawning.avi_289.jpg\n",
      "../Data/VideoFrames/47-MaleNoGlasses-Normal.avi/47-MaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/47-MaleNoGlasses-Yawning.avi/47-MaleNoGlasses-Yawning.avi_290.jpg\n",
      "../Data/VideoFrames/1-FemaleNoGlasses-Normal.avi/1-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/3-FemaleGlasses-Yawning.avi/3-FemaleGlasses-Yawning.avi_070.jpg\n",
      "../Data/VideoFrames/6-FemaleNoGlasses-Normal.avi/6-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/8-FemaleGlasses-Yawning.avi/8-FemaleGlasses-Yawning.avi_110.jpg\n",
      "../Data/VideoFrames/11-FemaleNoGlasses-Normal.avi/11-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/13-FemaleNoGlasses-Yawning.avi/13-FemaleNoGlasses-Yawning.avi_250.jpg\n",
      "../Data/VideoFrames/15-FemaleSunGlasses-Normal.avi/15-FemaleSunGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/17-FemaleNoGlasses-Yawning.avi/17-FemaleNoGlasses-Yawning.avi_055.jpg\n",
      "../Data/VideoFrames/18-FemaleSunGlasses-Normal.avi/18-FemaleSunGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/20-FemaleNoGlasses-Yawning.avi/20-FemaleNoGlasses-Yawning.avi_205.jpg\n",
      "../Data/VideoFrames/22-FemaleSunGlasses-Normal.avi/22-FemaleSunGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/24-FemaleNoGlasses-Normal.avi/24-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/25-FemaleSunGlasses-Yawning.avi/25-FemaleSunGlasses-Yawning.avi_065.jpg\n",
      "../Data/VideoFrames/27-FemaleNoGlasses-Normal.avi/27-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/28-FemaleNoGlasses-Yawning.avi/28-FemaleNoGlasses-Yawning.avi_120.jpg\n",
      "../Data/VideoFrames/31-FemaleGlasses-Normal.avi/31-FemaleGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/32-FemaleSunGlasses-Yawning.avi/32-FemaleSunGlasses-Yawning.avi_520.jpg\n",
      "../Data/VideoFrames/34-FemaleNoGlasses-Yawning.avi/34-FemaleNoGlasses-Yawning.avi_150.jpg\n",
      "../Data/VideoFrames/36-FemaleNoGlasses-Talking&Yawning.avi/36-FemaleNoGlasses-Talking&Yawning.avi_020.jpg\n",
      "../Data/VideoFrames/38-FemaleNoGlasses-Normal.avi/38-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/40-FemaleNoGlasses-Normal.avi/40-FemaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/42-FemaleSunGlasses-Yawning.avi/42-FemaleSunGlasses-Yawning.avi_260.jpg\n",
      "../Data/VideoFrames/1-MaleSunGlasses-Yawning.avi/1-MaleSunGlasses-Yawning.avi_145.jpg\n",
      "../Data/VideoFrames/3-MaleNoGlasses-Normal.avi/3-MaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/5-MaleNoGlasses-Yawning.avi/5-MaleNoGlasses-Yawning.avi_135.jpg\n",
      "../Data/VideoFrames/8-MaleGlassesBeard-Normal.avi/8-MaleGlassesBeard-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/10-MaleNoGlasses-Yawning.avi/10-MaleNoGlasses-Yawning.avi_025.jpg\n",
      "../Data/VideoFrames/13-MaleGlasses-Normal.avi/13-MaleGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/15-MaleNoGlasses-Yawning.avi/15-MaleNoGlasses-Yawning.avi_090.jpg\n",
      "../Data/VideoFrames/18-MaleNoGlasses-Yawning.avi/18-MaleNoGlasses-Yawning.avi_175.jpg\n",
      "../Data/VideoFrames/21-MaleGlasses-Normal.avi/21-MaleGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/23-MaleGlassesBeard-Talking&Yawning.avi/23-MaleGlassesBeard-Talking&Yawning.avi_020.jpg\n",
      "../Data/VideoFrames/24-MaleGlasses-Yawning.avi/24-MaleGlasses-Yawning.avi_260.jpg\n",
      "../Data/VideoFrames/26-MaleNoGlasses-Normal.avi/26-MaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/27-MaleNoGlasses-Yawning.avi/27-MaleNoGlasses-Yawning.avi_180.jpg\n",
      "../Data/VideoFrames/29-MaleNoGlasses-Normal.avi/29-MaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/31-MaleGlasses-Normal.avi/31-MaleGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/33-MaleGlasses-Normal.avi/33-MaleGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/35-MaleNoGlasses-Normal.avi/35-MaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/36-MaleSunGlasses-Yawning.avi/36-MaleSunGlasses-Yawning.avi_200.jpg\n",
      "../Data/VideoFrames/38-MaleSunGlasses-Normal.avi/38-MaleSunGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/40-MaleNoGlasses-Yawning.avi/40-MaleNoGlasses-Yawning.avi_215.jpg\n",
      "../Data/VideoFrames/43-MaleNoGlasses-Normal.avi/43-MaleNoGlasses-Normal.avi_020.jpg\n",
      "../Data/VideoFrames/45-MaleNoGlasses-Yawning.avi/45-MaleNoGlasses-Yawning.avi_175.jpg\n",
      "Frame features in train set: (176, 20, 2048)\n",
      "Frame masks in train set: (176, 20)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video-name\"].values.tolist()\n",
    "    start_nums = df[\"yawn-start\"].values.tolist()\n",
    "    # print(video_paths)\n",
    "    labels = df[\"Action\"].values\n",
    "    labels = label_processor(labels[..., None]).numpy()\n",
    "\n",
    "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
    "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
    "    # masked with padding or not.\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        videoName = path\n",
    "        number = start_nums[idx]\n",
    "        frames = []\n",
    "\n",
    "        while(len(frames)<=MAX_SEQ_LENGTH):\n",
    "            path = \"../Data/VideoFrames/\"+videoName+\"/\"+videoName+\"_\"+f\"{number:03}\"+\".jpg\"\n",
    "            frames.append(image.load_img(path, target_size=(224, 224, 3)))\n",
    "            number+=1\n",
    "        print(path)\n",
    "\n",
    "\n",
    "        frames = load_video(frames)\n",
    "        frames = frames[None, ...]\n",
    "        # Initialize placeholders to store the masks and features of the current video.\n",
    "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "        # Extract features from the frames of the current video.\n",
    "        \n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :]\n",
    "                )\n",
    "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "\n",
    "    return (frame_features, frame_masks), labels\n",
    "\n",
    "\n",
    "train_data, train_labels = prepare_all_videos(dfTrain, \"../Data/Mirror\")\n",
    "test_data, test_labels = prepare_all_videos(dfTest, \"../Data/Mirror\")\n",
    "\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features in train set: (44, 20, 2048)\n",
      "Frame masks in train set: (44, 20)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Frame features in train set: {test_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {test_data[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4/4 [==============================] - ETA: 0s - loss: 1.3407 - accuracy: 0.3740\n",
      "Epoch 1: val_loss improved from inf to 1.22618, saving model to .\\video_classifier\n",
      "4/4 [==============================] - 16s 1s/step - loss: 1.3407 - accuracy: 0.3740 - val_loss: 1.2262 - val_accuracy: 0.4906\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - ETA: 0s - loss: 1.1761 - accuracy: 0.4715\n",
      "Epoch 2: val_loss improved from 1.22618 to 1.07359, saving model to .\\video_classifier\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 1.1761 - accuracy: 0.4715 - val_loss: 1.0736 - val_accuracy: 0.4528\n",
      "Epoch 3/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.0537 - accuracy: 0.4688\n",
      "Epoch 3: val_loss improved from 1.07359 to 0.97663, saving model to .\\video_classifier\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 1.0546 - accuracy: 0.4797 - val_loss: 0.9766 - val_accuracy: 0.4528\n",
      "Epoch 4/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9749 - accuracy: 0.4583\n",
      "Epoch 4: val_loss improved from 0.97663 to 0.93079, saving model to .\\video_classifier\n",
      "4/4 [==============================] - 0s 71ms/step - loss: 0.9849 - accuracy: 0.4797 - val_loss: 0.9308 - val_accuracy: 0.4528\n",
      "Epoch 5/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9744 - accuracy: 0.4896\n",
      "Epoch 5: val_loss improved from 0.93079 to 0.91211, saving model to .\\video_classifier\n",
      "4/4 [==============================] - 0s 74ms/step - loss: 0.9530 - accuracy: 0.4797 - val_loss: 0.9121 - val_accuracy: 0.4528\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.9458 - accuracy: 0.4797\n",
      "Epoch 6: val_loss improved from 0.91211 to 0.90420, saving model to .\\video_classifier\n",
      "4/4 [==============================] - 0s 121ms/step - loss: 0.9458 - accuracy: 0.4797 - val_loss: 0.9042 - val_accuracy: 0.4528\n",
      "Epoch 7/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9147 - accuracy: 0.5104\n",
      "Epoch 7: val_loss improved from 0.90420 to 0.89809, saving model to .\\video_classifier\n",
      "4/4 [==============================] - 0s 71ms/step - loss: 0.9421 - accuracy: 0.4797 - val_loss: 0.8981 - val_accuracy: 0.4528\n",
      "Epoch 8/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9174 - accuracy: 0.4896\n",
      "Epoch 8: val_loss improved from 0.89809 to 0.89471, saving model to .\\video_classifier\n",
      "4/4 [==============================] - 0s 71ms/step - loss: 0.9426 - accuracy: 0.4797 - val_loss: 0.8947 - val_accuracy: 0.4528\n",
      "Epoch 9/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9673 - accuracy: 0.4792\n",
      "Epoch 9: val_loss improved from 0.89471 to 0.89054, saving model to .\\video_classifier\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 0.9398 - accuracy: 0.4797 - val_loss: 0.8905 - val_accuracy: 0.4528\n",
      "Epoch 10/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9054 - accuracy: 0.3854\n",
      "Epoch 10: val_loss improved from 0.89054 to 0.88823, saving model to .\\video_classifier\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.9397 - accuracy: 0.3821 - val_loss: 0.8882 - val_accuracy: 0.4528\n",
      "Epoch 11/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9016 - accuracy: 0.4896\n",
      "Epoch 11: val_loss improved from 0.88823 to 0.88707, saving model to .\\video_classifier\n",
      "4/4 [==============================] - 0s 73ms/step - loss: 0.9363 - accuracy: 0.4797 - val_loss: 0.8871 - val_accuracy: 0.4528\n",
      "Epoch 12/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9110 - accuracy: 0.5000\n",
      "Epoch 12: val_loss improved from 0.88707 to 0.88690, saving model to .\\video_classifier\n",
      "4/4 [==============================] - 0s 75ms/step - loss: 0.9334 - accuracy: 0.4797 - val_loss: 0.8869 - val_accuracy: 0.4528\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.9324 - accuracy: 0.4797\n",
      "Epoch 13: val_loss did not improve from 0.88690\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.9324 - accuracy: 0.4797 - val_loss: 0.8870 - val_accuracy: 0.4528\n",
      "Epoch 14/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9088 - accuracy: 0.5312\n",
      "Epoch 14: val_loss improved from 0.88690 to 0.88677, saving model to .\\video_classifier\n",
      "4/4 [==============================] - 0s 78ms/step - loss: 0.9319 - accuracy: 0.4797 - val_loss: 0.8868 - val_accuracy: 0.4528\n",
      "Epoch 15/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9742 - accuracy: 0.4896\n",
      "Epoch 15: val_loss improved from 0.88677 to 0.88599, saving model to .\\video_classifier\n",
      "4/4 [==============================] - 0s 72ms/step - loss: 0.9312 - accuracy: 0.4797 - val_loss: 0.8860 - val_accuracy: 0.4528\n",
      "Epoch 16/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.8566 - accuracy: 0.4896\n",
      "Epoch 16: val_loss improved from 0.88599 to 0.88433, saving model to .\\video_classifier\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.9315 - accuracy: 0.4634 - val_loss: 0.8843 - val_accuracy: 0.4906\n",
      "Epoch 17/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9339 - accuracy: 0.4583\n",
      "Epoch 17: val_loss did not improve from 0.88433\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.9302 - accuracy: 0.4715 - val_loss: 0.8851 - val_accuracy: 0.4528\n",
      "Epoch 18/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9157 - accuracy: 0.4688\n",
      "Epoch 18: val_loss did not improve from 0.88433\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.9304 - accuracy: 0.4797 - val_loss: 0.8863 - val_accuracy: 0.4528\n",
      "Epoch 19/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9553 - accuracy: 0.4688\n",
      "Epoch 19: val_loss did not improve from 0.88433\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.9309 - accuracy: 0.4797 - val_loss: 0.8898 - val_accuracy: 0.4528\n",
      "Epoch 20/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9514 - accuracy: 0.5000\n",
      "Epoch 20: val_loss did not improve from 0.88433\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.9316 - accuracy: 0.4797 - val_loss: 0.8883 - val_accuracy: 0.4528\n",
      "Epoch 21/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9806 - accuracy: 0.4271\n",
      "Epoch 21: val_loss improved from 0.88433 to 0.88279, saving model to .\\video_classifier\n",
      "4/4 [==============================] - 0s 63ms/step - loss: 0.9357 - accuracy: 0.4065 - val_loss: 0.8828 - val_accuracy: 0.4906\n",
      "Epoch 22/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9540 - accuracy: 0.4688\n",
      "Epoch 22: val_loss did not improve from 0.88279\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.9321 - accuracy: 0.4472 - val_loss: 0.8846 - val_accuracy: 0.4528\n",
      "Epoch 23/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9288 - accuracy: 0.5312\n",
      "Epoch 23: val_loss did not improve from 0.88279\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.9318 - accuracy: 0.4797 - val_loss: 0.8857 - val_accuracy: 0.4528\n",
      "Epoch 24/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.8679 - accuracy: 0.5417\n",
      "Epoch 24: val_loss did not improve from 0.88279\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.9313 - accuracy: 0.4797 - val_loss: 0.8828 - val_accuracy: 0.4528\n",
      "Epoch 25/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9351 - accuracy: 0.4792\n",
      "Epoch 25: val_loss improved from 0.88279 to 0.88049, saving model to .\\video_classifier\n",
      "4/4 [==============================] - 0s 81ms/step - loss: 0.9312 - accuracy: 0.4715 - val_loss: 0.8805 - val_accuracy: 0.4906\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.9316 - accuracy: 0.4472\n",
      "Epoch 26: val_loss did not improve from 0.88049\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.9316 - accuracy: 0.4472 - val_loss: 0.8808 - val_accuracy: 0.4906\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.9316 - accuracy: 0.4390\n",
      "Epoch 27: val_loss did not improve from 0.88049\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.9316 - accuracy: 0.4390 - val_loss: 0.8859 - val_accuracy: 0.4528\n",
      "Epoch 28/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9738 - accuracy: 0.4792\n",
      "Epoch 28: val_loss did not improve from 0.88049\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.9309 - accuracy: 0.4797 - val_loss: 0.8876 - val_accuracy: 0.4528\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.9306 - accuracy: 0.4797\n",
      "Epoch 29: val_loss did not improve from 0.88049\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.9306 - accuracy: 0.4797 - val_loss: 0.8850 - val_accuracy: 0.4528\n",
      "Epoch 30/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9532 - accuracy: 0.4896\n",
      "Epoch 30: val_loss did not improve from 0.88049\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.9296 - accuracy: 0.4797 - val_loss: 0.8819 - val_accuracy: 0.4528\n",
      "Epoch 31/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9538 - accuracy: 0.4896\n",
      "Epoch 31: val_loss did not improve from 0.88049\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.9295 - accuracy: 0.4797 - val_loss: 0.8811 - val_accuracy: 0.4906\n",
      "Epoch 32/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9610 - accuracy: 0.4375\n",
      "Epoch 32: val_loss improved from 0.88049 to 0.88042, saving model to .\\video_classifier\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 0.9351 - accuracy: 0.4472 - val_loss: 0.8804 - val_accuracy: 0.4906\n",
      "Epoch 33/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9320 - accuracy: 0.5000\n",
      "Epoch 33: val_loss did not improve from 0.88042\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.9351 - accuracy: 0.4472 - val_loss: 0.8864 - val_accuracy: 0.4528\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.9317 - accuracy: 0.4797\n",
      "Epoch 34: val_loss did not improve from 0.88042\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.9317 - accuracy: 0.4797 - val_loss: 0.8871 - val_accuracy: 0.4528\n",
      "Epoch 35/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9133 - accuracy: 0.4896\n",
      "Epoch 35: val_loss did not improve from 0.88042\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.9301 - accuracy: 0.4797 - val_loss: 0.8811 - val_accuracy: 0.4528\n",
      "Epoch 36/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.8968 - accuracy: 0.4062\n",
      "Epoch 36: val_loss improved from 0.88042 to 0.87981, saving model to .\\video_classifier\n",
      "4/4 [==============================] - 0s 61ms/step - loss: 0.9330 - accuracy: 0.4228 - val_loss: 0.8798 - val_accuracy: 0.4906\n",
      "Epoch 37/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9583 - accuracy: 0.4062\n",
      "Epoch 37: val_loss did not improve from 0.87981\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.9337 - accuracy: 0.3984 - val_loss: 0.8823 - val_accuracy: 0.4528\n",
      "Epoch 38/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9552 - accuracy: 0.5000\n",
      "Epoch 38: val_loss did not improve from 0.87981\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.9351 - accuracy: 0.4797 - val_loss: 0.8884 - val_accuracy: 0.4528\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.9344 - accuracy: 0.4797\n",
      "Epoch 39: val_loss did not improve from 0.87981\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.9344 - accuracy: 0.4797 - val_loss: 0.8812 - val_accuracy: 0.4528\n",
      "Epoch 40/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9149 - accuracy: 0.3854\n",
      "Epoch 40: val_loss did not improve from 0.87981\n",
      "4/4 [==============================] - 0s 59ms/step - loss: 0.9323 - accuracy: 0.3821 - val_loss: 0.8824 - val_accuracy: 0.4528\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.9297 - accuracy: 0.4797\n",
      "Epoch 41: val_loss did not improve from 0.87981\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.9297 - accuracy: 0.4797 - val_loss: 0.8819 - val_accuracy: 0.4528\n",
      "Epoch 42/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9538 - accuracy: 0.5208\n",
      "Epoch 42: val_loss did not improve from 0.87981\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.9373 - accuracy: 0.4797 - val_loss: 0.8864 - val_accuracy: 0.4528\n",
      "Epoch 43/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9564 - accuracy: 0.4479\n",
      "Epoch 43: val_loss did not improve from 0.87981\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.9326 - accuracy: 0.4309 - val_loss: 0.8800 - val_accuracy: 0.4906\n",
      "Epoch 44/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9166 - accuracy: 0.4583\n",
      "Epoch 44: val_loss did not improve from 0.87981\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.9330 - accuracy: 0.4472 - val_loss: 0.8798 - val_accuracy: 0.4906\n",
      "Epoch 45/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9337 - accuracy: 0.4896\n",
      "Epoch 45: val_loss did not improve from 0.87981\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.9311 - accuracy: 0.4715 - val_loss: 0.8860 - val_accuracy: 0.4528\n",
      "Epoch 46/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9431 - accuracy: 0.5417\n",
      "Epoch 46: val_loss did not improve from 0.87981\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.9399 - accuracy: 0.4797 - val_loss: 0.8941 - val_accuracy: 0.4528\n",
      "Epoch 47/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9398 - accuracy: 0.4479\n",
      "Epoch 47: val_loss did not improve from 0.87981\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.9361 - accuracy: 0.4797 - val_loss: 0.8803 - val_accuracy: 0.4906\n",
      "Epoch 48/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9758 - accuracy: 0.4688\n",
      "Epoch 48: val_loss did not improve from 0.87981\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.9370 - accuracy: 0.4472 - val_loss: 0.8800 - val_accuracy: 0.4906\n",
      "Epoch 49/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9342 - accuracy: 0.4688\n",
      "Epoch 49: val_loss did not improve from 0.87981\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.9294 - accuracy: 0.4878 - val_loss: 0.8832 - val_accuracy: 0.4528\n",
      "Epoch 50/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.8575 - accuracy: 0.5208\n",
      "Epoch 50: val_loss did not improve from 0.87981\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.9468 - accuracy: 0.4797 - val_loss: 0.8981 - val_accuracy: 0.4528\n",
      "Epoch 51/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9847 - accuracy: 0.4583\n",
      "Epoch 51: val_loss did not improve from 0.87981\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.9353 - accuracy: 0.4797 - val_loss: 0.8807 - val_accuracy: 0.4528\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.9320 - accuracy: 0.4472\n",
      "Epoch 52: val_loss improved from 0.87981 to 0.87920, saving model to .\\video_classifier\n",
      "4/4 [==============================] - 0s 59ms/step - loss: 0.9320 - accuracy: 0.4472 - val_loss: 0.8792 - val_accuracy: 0.4906\n",
      "Epoch 53/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9576 - accuracy: 0.4271\n",
      "Epoch 53: val_loss did not improve from 0.87920\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.9320 - accuracy: 0.4472 - val_loss: 0.8804 - val_accuracy: 0.4906\n",
      "Epoch 54/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9137 - accuracy: 0.5000\n",
      "Epoch 54: val_loss did not improve from 0.87920\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.9296 - accuracy: 0.4959 - val_loss: 0.8829 - val_accuracy: 0.4528\n",
      "Epoch 55/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9554 - accuracy: 0.4583\n",
      "Epoch 55: val_loss did not improve from 0.87920\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.9294 - accuracy: 0.4797 - val_loss: 0.8852 - val_accuracy: 0.4528\n",
      "Epoch 56/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9590 - accuracy: 0.4375\n",
      "Epoch 56: val_loss did not improve from 0.87920\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.9328 - accuracy: 0.4797 - val_loss: 0.8825 - val_accuracy: 0.4528\n",
      "Epoch 57/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9306 - accuracy: 0.5104\n",
      "Epoch 57: val_loss did not improve from 0.87920\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.9312 - accuracy: 0.4797 - val_loss: 0.8873 - val_accuracy: 0.4528\n",
      "Epoch 58/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9365 - accuracy: 0.4792\n",
      "Epoch 58: val_loss did not improve from 0.87920\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.9315 - accuracy: 0.4797 - val_loss: 0.8838 - val_accuracy: 0.4528\n",
      "Epoch 59/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9311 - accuracy: 0.5104\n",
      "Epoch 59: val_loss did not improve from 0.87920\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.9306 - accuracy: 0.4797 - val_loss: 0.8821 - val_accuracy: 0.4528\n",
      "Epoch 60/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9356 - accuracy: 0.4271\n",
      "Epoch 60: val_loss improved from 0.87920 to 0.87914, saving model to .\\video_classifier\n",
      "4/4 [==============================] - 0s 59ms/step - loss: 0.9306 - accuracy: 0.4472 - val_loss: 0.8791 - val_accuracy: 0.4906\n",
      "Epoch 61/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9154 - accuracy: 0.4271\n",
      "Epoch 61: val_loss did not improve from 0.87914\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.9320 - accuracy: 0.4228 - val_loss: 0.8804 - val_accuracy: 0.4528\n",
      "Epoch 62/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9147 - accuracy: 0.4792\n",
      "Epoch 62: val_loss did not improve from 0.87914\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.9316 - accuracy: 0.4797 - val_loss: 0.8824 - val_accuracy: 0.4528\n",
      "Epoch 63/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9153 - accuracy: 0.4792\n",
      "Epoch 63: val_loss did not improve from 0.87914\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 0.9308 - accuracy: 0.4797 - val_loss: 0.8821 - val_accuracy: 0.4528\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.9311 - accuracy: 0.4797\n",
      "Epoch 64: val_loss did not improve from 0.87914\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.9311 - accuracy: 0.4797 - val_loss: 0.8807 - val_accuracy: 0.4528\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.9304 - accuracy: 0.4309\n",
      "Epoch 65: val_loss did not improve from 0.87914\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.9304 - accuracy: 0.4309 - val_loss: 0.8817 - val_accuracy: 0.4528\n",
      "Epoch 66/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.8913 - accuracy: 0.4896\n",
      "Epoch 66: val_loss did not improve from 0.87914\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.9300 - accuracy: 0.4797 - val_loss: 0.8854 - val_accuracy: 0.4528\n",
      "Epoch 67/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9621 - accuracy: 0.4167\n",
      "Epoch 67: val_loss did not improve from 0.87914\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.9381 - accuracy: 0.3821 - val_loss: 0.8809 - val_accuracy: 0.4906\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.9317 - accuracy: 0.4634\n",
      "Epoch 68: val_loss did not improve from 0.87914\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.9317 - accuracy: 0.4634 - val_loss: 0.8854 - val_accuracy: 0.4528\n",
      "Epoch 69/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9374 - accuracy: 0.5000\n",
      "Epoch 69: val_loss did not improve from 0.87914\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.9383 - accuracy: 0.4797 - val_loss: 0.8913 - val_accuracy: 0.4528\n",
      "Epoch 70/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9625 - accuracy: 0.4375\n",
      "Epoch 70: val_loss did not improve from 0.87914\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.9393 - accuracy: 0.4146 - val_loss: 0.8801 - val_accuracy: 0.4906\n",
      "Epoch 71/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9127 - accuracy: 0.4792\n",
      "Epoch 71: val_loss did not improve from 0.87914\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.9340 - accuracy: 0.4472 - val_loss: 0.8799 - val_accuracy: 0.4906\n",
      "Epoch 72/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9129 - accuracy: 0.4688\n",
      "Epoch 72: val_loss did not improve from 0.87914\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.9348 - accuracy: 0.4472 - val_loss: 0.8937 - val_accuracy: 0.4528\n",
      "Epoch 73/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.8997 - accuracy: 0.4792\n",
      "Epoch 73: val_loss did not improve from 0.87914\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.9341 - accuracy: 0.4797 - val_loss: 0.8853 - val_accuracy: 0.4528\n",
      "Epoch 74/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9761 - accuracy: 0.4479\n",
      "Epoch 74: val_loss did not improve from 0.87914\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.9319 - accuracy: 0.4390 - val_loss: 0.8805 - val_accuracy: 0.4906\n",
      "Epoch 75/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.8932 - accuracy: 0.4583\n",
      "Epoch 75: val_loss did not improve from 0.87914\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.9311 - accuracy: 0.4472 - val_loss: 0.8801 - val_accuracy: 0.4906\n",
      "Epoch 76/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9591 - accuracy: 0.4271\n",
      "Epoch 76: val_loss did not improve from 0.87914\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.9331 - accuracy: 0.4472 - val_loss: 0.8858 - val_accuracy: 0.4528\n",
      "Epoch 77/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9157 - accuracy: 0.4792\n",
      "Epoch 77: val_loss did not improve from 0.87914\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.9313 - accuracy: 0.4797 - val_loss: 0.8827 - val_accuracy: 0.4528\n",
      "Epoch 78/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9388 - accuracy: 0.4062\n",
      "Epoch 78: val_loss did not improve from 0.87914\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.9386 - accuracy: 0.3740 - val_loss: 0.8795 - val_accuracy: 0.4906\n",
      "Epoch 79/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9147 - accuracy: 0.4792\n",
      "Epoch 79: val_loss did not improve from 0.87914\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.9294 - accuracy: 0.4959 - val_loss: 0.8872 - val_accuracy: 0.4528\n",
      "Epoch 80/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9596 - accuracy: 0.4688\n",
      "Epoch 80: val_loss did not improve from 0.87914\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.9332 - accuracy: 0.4797 - val_loss: 0.8896 - val_accuracy: 0.4528\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.9425 - accuracy: 0.4228\n",
      "Epoch 81: val_loss did not improve from 0.87914\n",
      "4/4 [==============================] - 0s 62ms/step - loss: 0.9425 - accuracy: 0.4228 - val_loss: 0.8813 - val_accuracy: 0.4906\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.9304 - accuracy: 0.4472\n",
      "Epoch 82: val_loss did not improve from 0.87914\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.9304 - accuracy: 0.4472 - val_loss: 0.8825 - val_accuracy: 0.4528\n",
      "Epoch 83/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9134 - accuracy: 0.4792\n",
      "Epoch 83: val_loss did not improve from 0.87914\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.9307 - accuracy: 0.4797 - val_loss: 0.8884 - val_accuracy: 0.4528\n",
      "Epoch 84/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9203 - accuracy: 0.4583\n",
      "Epoch 84: val_loss did not improve from 0.87914\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.9339 - accuracy: 0.4797 - val_loss: 0.8821 - val_accuracy: 0.4528\n",
      "Epoch 85/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9786 - accuracy: 0.4583\n",
      "Epoch 85: val_loss did not improve from 0.87914\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.9315 - accuracy: 0.4797 - val_loss: 0.8806 - val_accuracy: 0.4528\n",
      "Epoch 86/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.8953 - accuracy: 0.5208\n",
      "Epoch 86: val_loss did not improve from 0.87914\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.9384 - accuracy: 0.4797 - val_loss: 0.8847 - val_accuracy: 0.4528\n",
      "Epoch 87/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.8759 - accuracy: 0.4271\n",
      "Epoch 87: val_loss improved from 0.87914 to 0.87894, saving model to .\\video_classifier\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.9373 - accuracy: 0.4065 - val_loss: 0.8789 - val_accuracy: 0.4906\n",
      "Epoch 88/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9371 - accuracy: 0.4375\n",
      "Epoch 88: val_loss did not improve from 0.87894\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.9335 - accuracy: 0.4309 - val_loss: 0.8842 - val_accuracy: 0.4528\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.9323 - accuracy: 0.4797\n",
      "Epoch 89: val_loss did not improve from 0.87894\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.9323 - accuracy: 0.4797 - val_loss: 0.8883 - val_accuracy: 0.4528\n",
      "Epoch 90/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9375 - accuracy: 0.4583\n",
      "Epoch 90: val_loss did not improve from 0.87894\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.9319 - accuracy: 0.4797 - val_loss: 0.8813 - val_accuracy: 0.4906\n",
      "Epoch 91/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.8965 - accuracy: 0.4688\n",
      "Epoch 91: val_loss did not improve from 0.87894\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.9336 - accuracy: 0.4472 - val_loss: 0.8810 - val_accuracy: 0.4906\n",
      "Epoch 92/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9745 - accuracy: 0.4479\n",
      "Epoch 92: val_loss did not improve from 0.87894\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.9302 - accuracy: 0.4797 - val_loss: 0.8833 - val_accuracy: 0.4528\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.9323 - accuracy: 0.4797\n",
      "Epoch 93: val_loss did not improve from 0.87894\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.9323 - accuracy: 0.4797 - val_loss: 0.8861 - val_accuracy: 0.4528\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.9312 - accuracy: 0.4797\n",
      "Epoch 94: val_loss did not improve from 0.87894\n",
      "4/4 [==============================] - 0s 62ms/step - loss: 0.9312 - accuracy: 0.4797 - val_loss: 0.8829 - val_accuracy: 0.4528\n",
      "Epoch 95/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9304 - accuracy: 0.5208\n",
      "Epoch 95: val_loss did not improve from 0.87894\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 0.9324 - accuracy: 0.4797 - val_loss: 0.8841 - val_accuracy: 0.4528\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.9318 - accuracy: 0.4553\n",
      "Epoch 96: val_loss did not improve from 0.87894\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.9318 - accuracy: 0.4553 - val_loss: 0.8792 - val_accuracy: 0.4906\n",
      "Epoch 97/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9341 - accuracy: 0.4896\n",
      "Epoch 97: val_loss did not improve from 0.87894\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.9338 - accuracy: 0.4634 - val_loss: 0.8837 - val_accuracy: 0.4528\n",
      "Epoch 98/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9570 - accuracy: 0.4479\n",
      "Epoch 98: val_loss did not improve from 0.87894\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.9326 - accuracy: 0.4309 - val_loss: 0.8817 - val_accuracy: 0.4528\n",
      "Epoch 99/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.8931 - accuracy: 0.5000\n",
      "Epoch 99: val_loss did not improve from 0.87894\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.9330 - accuracy: 0.4797 - val_loss: 0.8876 - val_accuracy: 0.4528\n",
      "Epoch 100/100\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9562 - accuracy: 0.4792\n",
      "Epoch 100: val_loss did not improve from 0.87894\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.9313 - accuracy: 0.4797 - val_loss: 0.8816 - val_accuracy: 0.4528\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.8606 - accuracy: 0.4545\n",
      "Test accuracy: 45.45%\n"
     ]
    }
   ],
   "source": [
    "# Utility for our sequence model.\n",
    "def get_sequence_model():\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "    x = keras.layers.GRU(16, return_sequences=True)(\n",
    "        frame_features_input, mask=mask_input\n",
    "    )\n",
    "    x = keras.layers.GRU(8)(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Adam(epsilon=1, learning_rate=5), metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return rnn_model\n",
    "\n",
    "\n",
    "# Utility for running experiments.\n",
    "def run_experiment():\n",
    "    filepath = \"./video_classifier\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1]],\n",
    "        train_labels,\n",
    "        validation_split=0.3,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "\n",
    "    seq_model.load_weights(filepath)\n",
    "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history, seq_model\n",
    "\n",
    "\n",
    "_, sequence_model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test video path: 36-FemaleNoGlasses-Talking&Yawning.avi\n",
      "0\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "  Yawning: 49.06%\n",
      "  Normal: 44.10%\n",
      "  Talking&Yawning:  6.04%\n",
      "  talking:  0.80%\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Image data must be a sequence of ndimages.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [19], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTest video path: \u001b[39m\u001b[39m{\u001b[39;00mtest_video\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m test_frames \u001b[39m=\u001b[39m sequence_prediction(test_video)\n\u001b[1;32m---> 41\u001b[0m to_gif(test_frames[:MAX_SEQ_LENGTH])\n",
      "Cell \u001b[1;32mIn [19], line 34\u001b[0m, in \u001b[0;36mto_gif\u001b[1;34m(images)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_gif\u001b[39m(images):\n\u001b[0;32m     33\u001b[0m     converted_images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39muint8)\n\u001b[1;32m---> 34\u001b[0m     imageio\u001b[39m.\u001b[39;49mmimsave(\u001b[39m\"\u001b[39;49m\u001b[39manimation.gif\u001b[39;49m\u001b[39m\"\u001b[39;49m, converted_images, fps\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[0;32m     35\u001b[0m     \u001b[39mreturn\u001b[39;00m embed\u001b[39m.\u001b[39membed_file(\u001b[39m\"\u001b[39m\u001b[39manimation.gif\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\imageio\\v2.py:357\u001b[0m, in \u001b[0;36mmimwrite\u001b[1;34m(uri, ims, format, **kwargs)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[39m\"\"\"mimwrite(uri, ims, format=None, **kwargs)\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \n\u001b[0;32m    339\u001b[0m \u001b[39mWrite multiple images to the specified file.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[39m    to see what arguments are available for a particular format.\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_batch(ims):\n\u001b[1;32m--> 357\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImage data must be a sequence of ndimages.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    359\u001b[0m imopen_args \u001b[39m=\u001b[39m decypher_format_arg(\u001b[39mformat\u001b[39m)\n\u001b[0;32m    360\u001b[0m imopen_args[\u001b[39m\"\u001b[39m\u001b[39mlegacy_mode\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Image data must be a sequence of ndimages."
     ]
    }
   ],
   "source": [
    "\n",
    "def prepare_single_video(frames):\n",
    "    frames = frames[None, ...]\n",
    "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    for i, batch in enumerate(frames):\n",
    "        video_length = batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length)\n",
    "        print(length)\n",
    "        for j in range(length):\n",
    "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "    return frame_features, frame_mask\n",
    "\n",
    "\n",
    "def sequence_prediction(path):\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frames = load_video(os.path.join(\"../Data/Mirror/\", path))\n",
    "    frame_features, frame_mask = prepare_single_video(frames)\n",
    "    probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n",
    "\n",
    "    for i in np.argsort(probabilities)[::-1]:\n",
    "        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "    return frames\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def to_gif(images):\n",
    "    converted_images = images.astype(np.uint8)\n",
    "    imageio.mimsave(\"animation.gif\", converted_images, fps=10)\n",
    "    return embed.embed_file(\"animation.gif\")\n",
    "\n",
    "\n",
    "test_video = np.random.choice(dfTest[\"video-name\"].values.tolist())\n",
    "print(f\"Test video path: {test_video}\")\n",
    "test_frames = sequence_prediction(test_video)\n",
    "to_gif(test_frames[:MAX_SEQ_LENGTH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3deb5058a37522a2e4ac59617ed5a8c2cf9a3176becbc0158883759547460f28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
