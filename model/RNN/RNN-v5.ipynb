{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN-RNN using VGG16. RNN is created without GRU and trainned with 100 frames from each video.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import LSTM\n",
    "import numpy as np\n",
    "import glob,os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_W = 224\n",
    "IMG_H = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "MAX_SEQ_LENGTH = 100\n",
    "NUM_FEATURES = 2048\n",
    "\n",
    "df1 = pd.read_csv(\"../Data/mirror-data2.csv\")\n",
    "df1 = df1[df1.Action != (\"Talking\" or \"talking\")]\n",
    "df1.head()\n",
    "\n",
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(frames,  resize=(IMG_H, IMG_W)):\n",
    "    \n",
    "    frames = []\n",
    "    for frame in frames:\n",
    "\n",
    "        frame = crop_center_square(frame)\n",
    "        frame = cv2.resize(frame, resize)\n",
    "        frame = frame[:, :, [2, 1, 0]]\n",
    "        frames.append(frame)\n",
    "\n",
    "        \n",
    "    \n",
    "    return np.array(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_VGG16_model():\n",
    "  base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "  print(base_model.summary())\n",
    "  return base_model\n",
    "\n",
    "\n",
    "feature_extractor = load_VGG16_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_processor = keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=np.unique(df1['Action'])\n",
    ")\n",
    "print(label_processor.get_vocabulary())\n",
    "\n",
    "\n",
    "i = 0\n",
    "dfTrain = pd.DataFrame()\n",
    "dfTest = pd.DataFrame()\n",
    "\n",
    "while i<len(df1):\n",
    "    if i%5==0:\n",
    "        dfTest = dfTest.append(df1.iloc[[i]])\n",
    "    else :\n",
    "        dfTrain = dfTrain.append(df1.iloc[[i]])\n",
    "\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video-name\"].values.tolist()\n",
    "    start_nums = df[\"yawn-start\"].values.tolist()\n",
    "    # print(video_paths)\n",
    "    labels = df[\"Action\"].values\n",
    "    labels = label_processor(labels[..., None]).numpy()\n",
    "\n",
    "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
    "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
    "    # masked with padding or not.\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        videoName = path\n",
    "        number = start_nums[idx]\n",
    "        frames = []\n",
    "\n",
    "        while(len(frames)<=MAX_SEQ_LENGTH):\n",
    "            path = \"../Data/VideoFrames/\"+videoName+\"/\"+videoName+\"_\"+f\"{number:03}\"+\".jpg\"\n",
    "            frames.append(image.load_img(path, target_size=(224, 224, 3)))\n",
    "            number+=1\n",
    "        print(path)\n",
    "\n",
    "\n",
    "        frames = load_video(frames)\n",
    "        frames = frames[None, ...]\n",
    "        # Initialize placeholders to store the masks and features of the current video.\n",
    "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "        # Extract features from the frames of the current video.\n",
    "        \n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :]\n",
    "                )\n",
    "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "\n",
    "    return (frame_features, frame_masks), labels\n",
    "\n",
    "\n",
    "train_data, train_labels = prepare_all_videos(dfTrain, \"../Data/Mirror\")\n",
    "test_data, test_labels = prepare_all_videos(dfTest, \"../Data/Mirror\")\n",
    "\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data,train_labels,validation_data,validation_labels):\n",
    "  ''' used fully connected layers, SGD optimizer and \n",
    "      checkpoint to store the best weights'''\n",
    "  frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "  mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(256,dropout=0.2,input_shape=(MAX_SEQ_LENGTH, NUM_FEATURES)))\n",
    "  model.add(Dense(1024, activation='relu'))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(5, activation='softmax'))\n",
    "  sgd = SGD(lr=0.00005, decay = 1e-6, momentum=0.9, nesterov=True)\n",
    "  model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "  #model.load_weights('video_1_LSTM_1_512.h5')\n",
    "  #callbacks = [ EarlyStopping(monitor='val_loss', patience=10, verbose=0), ModelCheckpoint('video_1_LSTM_1_1024.h5', monitor='val_loss', save_best_only=True, verbose=0) ]\n",
    "  nb_epoch = 500\n",
    "  model.fit(train_data,train_labels,validation_data=(validation_data,validation_labels),shuffle=True,verbose=1, epochs = 15)\n",
    "  return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(train_data[0], train_labels, test_data[0], test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_whole_videos(train_data,train_labels,validation_data,validation_labels):\n",
    "  parent = os.listdir(\"/Users/.../video/test\")\n",
    "  #.....................................Testing on whole videos.................................................................\n",
    "  x = []\n",
    "  y = []\n",
    "  count = 0\n",
    "  output = 0\n",
    "  count_video = 0\n",
    "  correct_video = 0\n",
    "  total_video = 0\n",
    "  base_model = load_VGG16_model()\n",
    "  model = train_model(train_data,train_labels,validation_data,validation_labels)\n",
    "  for video_class in parent[1:]:\n",
    "      print video_class\n",
    "      child = os.listdir(\"/Users/.../video/test\" + \"/\" + video_class)\n",
    "      for class_i in child[1:]:\n",
    "          sub_child = os.listdir(\"/Users/.../video/test\" + \"/\" + video_class + \"/\" + class_i)\n",
    "          for image_fol in sub_child[1:]:\n",
    "              if (video_class ==  'class_4' ):\n",
    "                  if(count%4 == 0):\n",
    "                      image = imread(\"/Users/.../video/test\" + \"/\" + video_class + \"/\" + class_i + \"/\" + image_fol)\n",
    "                      image = imresize(image , (224,224))\n",
    "\n",
    "                      x.append(image)\n",
    "                      y.append(output)\n",
    "                      #cv2.imwrite('/Users/.../video/validate/' + video_class + '/' + str(count) + '_' + image_fol,image)\n",
    "                  count+=1\n",
    "\n",
    "              else:\n",
    "                  if(count%4 == 0):\n",
    "                      image = imread(\"/Users/.../video/test\" + \"/\" + video_class + \"/\" + class_i + \"/\" + image_fol)\n",
    "                      image = imresize(image , (224,224))\n",
    "                      x.append(image)\n",
    "                      y.append(output)\n",
    "                      #cv2.imwrite('/Users/.../video/validate/' + video_class + '/' + str(count) + '_' + image_fol,image)\n",
    "                  count+=1\n",
    "          #correct_video+=1\n",
    "          x = np.array(x)\n",
    "          y = np.array(y)\n",
    "          x_features = base_model.predict(x)\n",
    "          #np.save(open('feat_' + 'class_' + str(output) + '_' + str(count_video) +'_'  + '.npy','w'),x)\n",
    "\n",
    "          correct = 0\n",
    "          \n",
    "          answer = model.predict(x_features)\n",
    "          for i in range(len(answer)):\n",
    "              if(y[i] == np.argmax(answer[i])):\n",
    "                  correct+=1\n",
    "          print correct,\"correct\",len(answer)\n",
    "          total_video+=1\n",
    "          if(correct>= len(answer)/2):\n",
    "              correct_video+=1\n",
    "          x = []\n",
    "          y = []\n",
    "          count_video+=1\n",
    "      output+=1\n",
    "\n",
    "  print(\"correct_video\",correct_video,\"total_video\",total_video)\n",
    "  print(\"The accuracy for video classification of \",total_video, \" videos is \", (correct_video/total_video))\n",
    "  \n",
    "\n",
    "base_model = load_VGG16_model()\n",
    "train_data,train_labels,validation_data,validation_labels = extract_features_and_store(train_generator,validation_generator,base_model)\n",
    "train_model(train_data,train_labels,validation_data,validation_labels)\n",
    "test_on_whole_videos(train_data,train_labels,validation_data,validation_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3deb5058a37522a2e4ac59617ed5a8c2cf9a3176becbc0158883759547460f28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
