{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import imageio\n",
    "import medmnist\n",
    "import ipywidgets\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 128\n",
    "MAX_SEQ_LENGTH = 724\n",
    "\n",
    "# DATA\n",
    "BATCH_SIZE = 6\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (MAX_SEQ_LENGTH, IMG_SIZE, IMG_SIZE, 3)\n",
    "NUM_CLASSES = 1\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 25\n",
    "\n",
    "# TUBELET EMBEDDING\n",
    "PATCH_SIZE = (200, 32, 32)\n",
    "#NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "# ViViT ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "PROJECTION_DIM = 128\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Female Participant Number</th>\n",
       "      <th>Action</th>\n",
       "      <th>video-name</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>1-FemaleNoGlasses-Normal.avi</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Talking</td>\n",
       "      <td>1-FemaleNoGlasses-Talking.avi</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Yawning</td>\n",
       "      <td>1-FemaleNoGlasses-Yawning.avi</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Normal</td>\n",
       "      <td>2-FemaleNoGlasses-Normal.avi</td>\n",
       "      <td>C:\\Users\\bnich\\Documents\\_MyDocs\\_School\\2022-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Talking</td>\n",
       "      <td>2-FemaleNoGlasses-Talking.avi</td>\n",
       "      <td>C:\\Users\\bnich\\Documents\\_MyDocs\\_School\\2022-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Female Participant Number   Action                     video-name   \n",
       "0                          1   Normal   1-FemaleNoGlasses-Normal.avi  \\\n",
       "1                          1  Talking  1-FemaleNoGlasses-Talking.avi   \n",
       "2                          1  Yawning  1-FemaleNoGlasses-Yawning.avi   \n",
       "3                          2   Normal   2-FemaleNoGlasses-Normal.avi   \n",
       "4                          2  Talking  2-FemaleNoGlasses-Talking.avi   \n",
       "\n",
       "                                          Unnamed: 3  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3  C:\\Users\\bnich\\Documents\\_MyDocs\\_School\\2022-...  \n",
       "4  C:\\Users\\bnich\\Documents\\_MyDocs\\_School\\2022-...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"../data/female-mirror-data.csv\")\n",
    "df1[\"Action\"] = df1[\"Action\"].str.rstrip()\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"target\"] = (df1[\"Action\"]==\"Yawning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "dfTrain = pd.DataFrame()\n",
    "dfTest = pd.DataFrame()\n",
    "\n",
    "while i<len(df1):\n",
    "    if i%5==0:\n",
    "        dfTest = pd.concat([dfTest, df1.iloc[[i]]])\n",
    "    else :\n",
    "        dfTrain = pd.concat([dfTrain, df1.iloc[[i]]])\n",
    "\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTest.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain[dfTrain.target == True].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTest[dfTest.target == True].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_frames(frame, output_size):\n",
    "  \"\"\"\n",
    "    Pad and resize an image from a video.\n",
    "\n",
    "    Args:\n",
    "      frame: Image that needs to resized and padded. \n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      Formatted frame with padding of specified output size.\n",
    "  \"\"\"\n",
    "  frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
    "  frame = tf.image.resize_with_pad(frame, *output_size)\n",
    "  return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_from_video_file(video_path, MAX_SEQ_LENGTH, output_size = (IMG_SIZE, IMG_SIZE), frame_step = 1):\n",
    "  \"\"\"\n",
    "    Creates frames from each video file present for each category.\n",
    "\n",
    "    Args:\n",
    "      video_path: File path to the video.\n",
    "      n_frames: Number of frames to be created per video file.\n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
    "  \"\"\"\n",
    "  # Read each video frame by frame\n",
    "  result = []\n",
    "  \n",
    "  src = cv2.VideoCapture(str(video_path))  \n",
    "\n",
    "  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "  need_length = 1 + (MAX_SEQ_LENGTH - 1) * frame_step\n",
    "\n",
    "  if need_length > video_length:\n",
    "    start = 0\n",
    "  else:\n",
    "    max_start = video_length - need_length\n",
    "    start = random.randint(0, max_start + 1)\n",
    "\n",
    "  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "  # ret is a boolean indicating whether read was successful, frame is the image itself\n",
    "  ret, frame = src.read()\n",
    "  result.append(format_frames(frame, output_size))\n",
    "\n",
    "  for _ in range(MAX_SEQ_LENGTH - 1):\n",
    "    for _ in range(frame_step):\n",
    "      ret, frame = src.read()\n",
    "    if ret:\n",
    "      frame = format_frames(frame, output_size)\n",
    "      result.append(frame)\n",
    "    else:\n",
    "      result.append(np.zeros_like(result[0]))\n",
    "  src.release()\n",
    "  result = np.array(result)[..., [2, 1, 0]]\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameGenerator:\n",
    "  def __init__(self, df, root_dir, MAX_SEQ_LENGTH):\n",
    "    \"\"\" Returns a set of frames with their associated label. \n",
    "\n",
    "      Args:\n",
    "        path: Video file paths.\n",
    "        n_frames: Number of frames. \n",
    "        training: Boolean to determine if training dataset is being created.\n",
    "    \"\"\"\n",
    "    self.df = df\n",
    "    self.n_frames = MAX_SEQ_LENGTH\n",
    "    self.root_dir = root_dir\n",
    "\n",
    "  def __call__(self):\n",
    "    video_paths = self.df[\"video-name\"].values.tolist()\n",
    "    labels = self.df[\"target\"].values.tolist()\n",
    "\n",
    "    for idx, path in enumerate(video_paths):\n",
    "      video_frames = frames_from_video_file(os.path.join(self.root_dir, path), self.n_frames) \n",
    "      yield video_frames, labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n",
    "                    tf.TensorSpec(shape = (), dtype = tf.bool))\n",
    "train_ds = tf.data.Dataset.from_generator(FrameGenerator(dfTrain, \"../data/YawDD/YawDD dataset/Mirror/Female_mirror/\", MAX_SEQ_LENGTH),  output_signature = output_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.data.Dataset.from_generator(FrameGenerator(dfTest, \"../data/YawDD/YawDD dataset/Mirror/Female_mirror/\", MAX_SEQ_LENGTH),  output_signature = output_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)\n",
    "test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.batch(BATCH_SIZE)\n",
    "test_ds = test_ds.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TubeletEmbedding(layers.Layer):\n",
    "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.projection = layers.Conv3D(\n",
    "            filters=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=patch_size,\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
    "\n",
    "    def call(self, videos):\n",
    "        projected_patches = self.projection(videos)\n",
    "        flattened_patches = self.flatten(projected_patches)\n",
    "        return flattened_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, num_tokens, _ = input_shape\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_tokens, output_dim=self.embed_dim\n",
    "        )\n",
    "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
    "\n",
    "    def call(self, encoded_tokens):\n",
    "        # Encode the positions and add it to the encoded tokens\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_tokens = encoded_tokens + encoded_positions\n",
    "        return encoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vivit_classifier(\n",
    "    tubelet_embedder,\n",
    "    positional_encoder,\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    transformer_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embed_dim=PROJECTION_DIM,\n",
    "    layer_norm_eps=LAYER_NORM_EPS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "):\n",
    "    # Get the input layer\n",
    "    inputs = layers.Input(shape=(input_shape))\n",
    "    # Create patches.\n",
    "    patches = tubelet_embedder(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = positional_encoder(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization and MHSA\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=embed_dim * 4, activation=tf.nn.gelu),\n",
    "                layers.Dense(units=embed_dim, activation=tf.nn.gelu),\n",
    "            ]\n",
    "        )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
    "    representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(units=num_classes, activation=\"sigmoid\")(representation)\n",
    "\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "    # Initialize model\n",
    "    model = create_vivit_classifier(\n",
    "        tubelet_embedder=TubeletEmbedding(\n",
    "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
    "        ),\n",
    "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
    "    )\n",
    "\n",
    "    # Compile the model with the optimizer, loss function\n",
    "    # and the metrics.\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\n",
    "            \"accuracy\"\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Train the model.\n",
    "    _ = model.fit(train_ds, epochs=EPOCHS, validation_data = test_ds)\n",
    "\n",
    "    _, accuracy = model.evaluate(test_ds)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    #print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "21/21 [==============================] - 1788s 26s/step - loss: 0.6474 - accuracy: 0.6694 - val_loss: 0.6378 - val_accuracy: 0.7188\n",
      "Epoch 2/25\n",
      "21/21 [==============================] - 159s 8s/step - loss: 0.6021 - accuracy: 0.7016 - val_loss: 0.7235 - val_accuracy: 0.7188\n",
      "Epoch 3/25\n",
      "21/21 [==============================] - 200s 10s/step - loss: 0.5884 - accuracy: 0.7016 - val_loss: 0.7181 - val_accuracy: 0.7188\n",
      "Epoch 4/25\n",
      "21/21 [==============================] - 215s 10s/step - loss: 0.5941 - accuracy: 0.7016 - val_loss: 0.7401 - val_accuracy: 0.7188\n",
      "Epoch 5/25\n",
      "21/21 [==============================] - 198s 10s/step - loss: 0.5869 - accuracy: 0.7016 - val_loss: 0.7489 - val_accuracy: 0.7188\n",
      "Epoch 6/25\n",
      "21/21 [==============================] - 204s 10s/step - loss: 0.5772 - accuracy: 0.7016 - val_loss: 0.7744 - val_accuracy: 0.7188\n",
      "Epoch 7/25\n",
      "21/21 [==============================] - 192s 9s/step - loss: 0.5714 - accuracy: 0.7177 - val_loss: 0.7757 - val_accuracy: 0.7188\n",
      "Epoch 8/25\n",
      "21/21 [==============================] - 239s 11s/step - loss: 0.5702 - accuracy: 0.7177 - val_loss: 0.7929 - val_accuracy: 0.7188\n",
      "Epoch 9/25\n",
      "21/21 [==============================] - 245s 12s/step - loss: 0.5668 - accuracy: 0.7177 - val_loss: 0.8030 - val_accuracy: 0.7188\n",
      "Epoch 10/25\n",
      "21/21 [==============================] - 207s 10s/step - loss: 0.5599 - accuracy: 0.7258 - val_loss: 0.7905 - val_accuracy: 0.7188\n",
      "Epoch 11/25\n",
      "21/21 [==============================] - 215s 10s/step - loss: 0.5551 - accuracy: 0.7258 - val_loss: 0.8192 - val_accuracy: 0.7188\n",
      "Epoch 12/25\n",
      "21/21 [==============================] - 189s 9s/step - loss: 0.5519 - accuracy: 0.7661 - val_loss: 0.8115 - val_accuracy: 0.7188\n",
      "Epoch 13/25\n",
      "21/21 [==============================] - 213s 10s/step - loss: 0.5501 - accuracy: 0.7419 - val_loss: 0.8224 - val_accuracy: 0.7188\n",
      "Epoch 14/25\n",
      "21/21 [==============================] - 212s 10s/step - loss: 0.5455 - accuracy: 0.7581 - val_loss: 0.8457 - val_accuracy: 0.6875\n",
      "Epoch 15/25\n",
      "21/21 [==============================] - 239s 11s/step - loss: 0.5464 - accuracy: 0.7419 - val_loss: 0.8236 - val_accuracy: 0.7188\n",
      "Epoch 16/25\n",
      "21/21 [==============================] - 225s 11s/step - loss: 0.5463 - accuracy: 0.7581 - val_loss: 0.8252 - val_accuracy: 0.7188\n",
      "Epoch 17/25\n",
      "21/21 [==============================] - 213s 10s/step - loss: 0.5461 - accuracy: 0.7419 - val_loss: 0.8270 - val_accuracy: 0.7188\n",
      "Epoch 18/25\n",
      "21/21 [==============================] - 203s 10s/step - loss: 0.5428 - accuracy: 0.7661 - val_loss: 0.8482 - val_accuracy: 0.6875\n",
      "Epoch 19/25\n",
      "21/21 [==============================] - 223s 11s/step - loss: 0.5447 - accuracy: 0.7419 - val_loss: 0.8825 - val_accuracy: 0.6562\n",
      "Epoch 20/25\n",
      "21/21 [==============================] - 208s 10s/step - loss: 0.5427 - accuracy: 0.7500 - val_loss: 0.8559 - val_accuracy: 0.6875\n",
      "Epoch 21/25\n",
      "21/21 [==============================] - 221s 11s/step - loss: 0.5394 - accuracy: 0.7339 - val_loss: 0.8834 - val_accuracy: 0.6562\n",
      "Epoch 22/25\n",
      "21/21 [==============================] - 237s 11s/step - loss: 0.5324 - accuracy: 0.7419 - val_loss: 0.8514 - val_accuracy: 0.7188\n",
      "Epoch 23/25\n",
      "21/21 [==============================] - 235s 11s/step - loss: 0.5338 - accuracy: 0.7339 - val_loss: 0.8771 - val_accuracy: 0.6875\n",
      "Epoch 24/25\n",
      "21/21 [==============================] - 258s 13s/step - loss: 0.5308 - accuracy: 0.7581 - val_loss: 0.8822 - val_accuracy: 0.6875\n",
      "Epoch 25/25\n",
      "21/21 [==============================] - 249s 12s/step - loss: 0.5274 - accuracy: 0.7419 - val_loss: 0.8706 - val_accuracy: 0.6875\n",
      "6/6 [==============================] - 24s 4s/step - loss: 0.8706 - accuracy: 0.6875\n",
      "Test accuracy: 68.75%\n"
     ]
    }
   ],
   "source": [
    "trained_model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m NUM_SAMPLES_VIZ \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n\u001b[1;32m----> 2\u001b[0m t \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(test_ds)\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m):\n\u001b[0;32m      4\u001b[0m     testsamples, labels \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mget_next()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_ds' is not defined"
     ]
    }
   ],
   "source": [
    "NUM_SAMPLES_VIZ = 4\n",
    "t = iter(test_ds)\n",
    "for i in range(1):\n",
    "    testsamples, labels = t.get_next()\n",
    "testsamples, labels = testsamples[:NUM_SAMPLES_VIZ], labels[:NUM_SAMPLES_VIZ]\n",
    "\n",
    "ground_truths = []\n",
    "preds = []\n",
    "videos = []\n",
    "\n",
    "for i, (testsample, label) in enumerate(zip(testsamples, labels)):\n",
    "    # Generate gif\n",
    "    with io.BytesIO() as gif:\n",
    "        imageio.mimsave(gif, (testsample.numpy()*255).astype(\"uint8\"), \"GIF\")\n",
    "        videos.append(gif.getvalue())\n",
    "\n",
    "    # Get model prediction\n",
    "    output = trained_model.predict(tf.expand_dims(testsample, axis=0))[0]\n",
    "    pred = np.argmax(output, axis=0)\n",
    "\n",
    "    ground_truths.append(label.numpy().astype(\"int\"))\n",
    "    preds.append(pred)\n",
    "\n",
    "\n",
    "def make_box_for_grid(image_widget, fit):\n",
    "    \"\"\"Make a VBox to hold caption/image for demonstrating option_fit values.\n",
    "\n",
    "    Source: https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20Styling.html\n",
    "    \"\"\"\n",
    "    # Make the caption\n",
    "    if fit is not None:\n",
    "        fit_str = \"'{}'\".format(fit)\n",
    "    else:\n",
    "        fit_str = str(fit)\n",
    "\n",
    "    h = ipywidgets.HTML(value=\"\" + str(fit_str) + \"\")\n",
    "\n",
    "    # Make the green box with the image widget inside it\n",
    "    boxb = ipywidgets.widgets.Box()\n",
    "    boxb.children = [image_widget]\n",
    "\n",
    "    # Compose into a vertical box\n",
    "    vb = ipywidgets.widgets.VBox()\n",
    "    vb.layout.align_items = \"center\"\n",
    "    vb.children = [h, boxb]\n",
    "    return vb\n",
    "\n",
    "\n",
    "boxes = []\n",
    "#class_vocab = [\"Not Yawning\", \"Yawning\"]\n",
    "\n",
    "for i in range(NUM_SAMPLES_VIZ):\n",
    "    ib = ipywidgets.widgets.Image(value=videos[i], width=100, height=100)\n",
    "    true_class = str(ground_truths[i])\n",
    "    pred_class = str(preds[i])\n",
    "    caption = f\"T: {true_class} | P: {pred_class}\"\n",
    "\n",
    "    boxes.append(make_box_for_grid(ib, caption))\n",
    "\n",
    "ipywidgets.widgets.GridBox(\n",
    "    boxes, layout=ipywidgets.widgets.Layout(grid_template_columns=\"repeat(5, 200px)\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
