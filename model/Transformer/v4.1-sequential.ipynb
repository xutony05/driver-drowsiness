{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import imageio\n",
    "import medmnist\n",
    "import ipywidgets\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 32\n",
    "MAX_SEQ_LENGTH = 500\n",
    "\n",
    "# DATA\n",
    "BATCH_SIZE = 8\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (MAX_SEQ_LENGTH, IMG_SIZE, IMG_SIZE, 3)\n",
    "NUM_CLASSES = 1\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 100\n",
    "\n",
    "# TUBELET EMBEDDING\n",
    "PATCH_SIZE = (50, 8, 8)\n",
    "#NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "# ViViT ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "PROJECTION_DIM = 128\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Female Participant Number</th>\n",
       "      <th>Action</th>\n",
       "      <th>video-name</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>1-FemaleNoGlasses-Normal.avi</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Talking</td>\n",
       "      <td>1-FemaleNoGlasses-Talking.avi</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Yawning</td>\n",
       "      <td>1-FemaleNoGlasses-Yawning.avi</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Normal</td>\n",
       "      <td>2-FemaleNoGlasses-Normal.avi</td>\n",
       "      <td>C:\\Users\\bnich\\Documents\\_MyDocs\\_School\\2022-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Talking</td>\n",
       "      <td>2-FemaleNoGlasses-Talking.avi</td>\n",
       "      <td>C:\\Users\\bnich\\Documents\\_MyDocs\\_School\\2022-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Female Participant Number   Action                     video-name   \n",
       "0                          1   Normal   1-FemaleNoGlasses-Normal.avi  \\\n",
       "1                          1  Talking  1-FemaleNoGlasses-Talking.avi   \n",
       "2                          1  Yawning  1-FemaleNoGlasses-Yawning.avi   \n",
       "3                          2   Normal   2-FemaleNoGlasses-Normal.avi   \n",
       "4                          2  Talking  2-FemaleNoGlasses-Talking.avi   \n",
       "\n",
       "                                          Unnamed: 3  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3  C:\\Users\\bnich\\Documents\\_MyDocs\\_School\\2022-...  \n",
       "4  C:\\Users\\bnich\\Documents\\_MyDocs\\_School\\2022-...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"../data/female-mirror-data.csv\")\n",
    "df1[\"Action\"] = df1[\"Action\"].str.rstrip()\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"target\"] = (df1[\"Action\"]==\"Yawning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "dfTrain = pd.DataFrame()\n",
    "dfTest = pd.DataFrame()\n",
    "\n",
    "while i<len(df1):\n",
    "    if i%5==0:\n",
    "        dfTest = pd.concat([dfTest, df1.iloc[[i]]])\n",
    "    else :\n",
    "        dfTrain = pd.concat([dfTrain, df1.iloc[[i]]])\n",
    "\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTest.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain[dfTrain.target == True].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTest[dfTest.target == True].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "            #####################\n",
    "            ret, frame = cap.read() # read another frame\n",
    "            if not ret:\n",
    "                break\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "\n",
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video-name\"].values.tolist()\n",
    "    labels = df[\"target\"].values\n",
    "\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, IMG_SIZE, IMG_SIZE, 3), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        frames = load_video(os.path.join(root_dir, path)) / 255\n",
    "\n",
    "        if len(frames) < MAX_SEQ_LENGTH:\n",
    "            diff = MAX_SEQ_LENGTH - len(frames)\n",
    "            padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
    "            frames = np.concatenate((frames, padding))\n",
    "            \n",
    "        frames = frames[None, ...]\n",
    "        \n",
    "        frame_features[idx] = frames[0][0:MAX_SEQ_LENGTH]\n",
    "\n",
    "    return frame_features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = prepare_all_videos(dfTrain, \"../data/YawDD/YawDD dataset/Mirror/Female_mirror/\")\n",
    "test_data, test_labels = prepare_all_videos(dfTest, \"../data/YawDD/YawDD dataset/Mirror/Female_mirror/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features in train set: (124, 500, 32, 32, 3)\n",
      "Frame features in test set: (32, 500, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Frame features in train set: {train_data.shape}\")\n",
    "print(f\"Frame features in test set: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def preprocess(frames: tf.Tensor, label: tf.Tensor):\n",
    "    \"\"\"Preprocess the frames tensors and parse the labels.\"\"\"\n",
    "    # Preprocess images\n",
    "    frames = tf.image.convert_image_dtype(\n",
    "        frames[\n",
    "            ..., tf.newaxis\n",
    "        ],  # The new axis is to help for further processing with Conv3D layers\n",
    "        tf.float32,\n",
    "    )\n",
    "    # Parse label\n",
    "    #label = tf.cast(label, tf.float32)\n",
    "    return frames, label\n",
    "\n",
    "\n",
    "def prepare_dataloader(\n",
    "    videos: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    loader_type: str = \"train\",\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "):\n",
    "    \"\"\"Utility function to prepare the dataloader.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n",
    "\n",
    "    #if loader_type == \"train\":\n",
    "    #    dataset = dataset.shuffle(BATCH_SIZE * 2)\n",
    "\n",
    "    dataloader = (\n",
    "        #dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        dataset.batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = prepare_dataloader(train_data, train_labels)\n",
    "testloader = prepare_dataloader(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TubeletEmbedding(layers.Layer):\n",
    "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.projection = layers.Conv3D(\n",
    "            filters=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=patch_size,\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
    "\n",
    "    def call(self, videos):\n",
    "        projected_patches = self.projection(videos)\n",
    "        flattened_patches = self.flatten(projected_patches)\n",
    "        return flattened_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, num_tokens, _ = input_shape\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_tokens, output_dim=self.embed_dim\n",
    "        )\n",
    "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
    "\n",
    "    def call(self, encoded_tokens):\n",
    "        # Encode the positions and add it to the encoded tokens\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_tokens = encoded_tokens + encoded_positions\n",
    "        return encoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vivit_classifier(\n",
    "    tubelet_embedder,\n",
    "    positional_encoder,\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    transformer_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embed_dim=PROJECTION_DIM,\n",
    "    layer_norm_eps=LAYER_NORM_EPS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "):\n",
    "    # Get the input layer\n",
    "    inputs = layers.Input(shape=(input_shape))\n",
    "    # Create patches.\n",
    "    patches = tubelet_embedder(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = positional_encoder(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization and MHSA\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=embed_dim * 4, activation=tf.nn.gelu),\n",
    "                layers.Dense(units=embed_dim, activation=tf.nn.gelu),\n",
    "            ]\n",
    "        )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
    "    representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(units=num_classes, activation=\"sigmoid\")(representation)\n",
    "\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "    # Initialize model\n",
    "    model = create_vivit_classifier(\n",
    "        tubelet_embedder=TubeletEmbedding(\n",
    "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
    "        ),\n",
    "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
    "    )\n",
    "\n",
    "    # Compile the model with the optimizer, loss function\n",
    "    # and the metrics.\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\n",
    "            \"accuracy\"\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Train the model.\n",
    "    _ = model.fit(train_data, train_labels, epochs=EPOCHS, validation_split=0.15)\n",
    "\n",
    "    _, accuracy = model.evaluate(testloader)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    #print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4/4 [==============================] - 24s 1s/step - loss: 1.0008 - accuracy: 0.5524 - val_loss: 0.8216 - val_accuracy: 0.6316\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 3s 708ms/step - loss: 0.5927 - accuracy: 0.6857 - val_loss: 0.5757 - val_accuracy: 0.6842\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 3s 694ms/step - loss: 0.5621 - accuracy: 0.7143 - val_loss: 0.5105 - val_accuracy: 0.7895\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 3s 739ms/step - loss: 0.4962 - accuracy: 0.7524 - val_loss: 0.5770 - val_accuracy: 0.6316\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 3s 700ms/step - loss: 0.5342 - accuracy: 0.7238 - val_loss: 0.5351 - val_accuracy: 0.6842\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 3s 744ms/step - loss: 0.4876 - accuracy: 0.7524 - val_loss: 0.4701 - val_accuracy: 0.6842\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 3s 729ms/step - loss: 0.4727 - accuracy: 0.7429 - val_loss: 0.4677 - val_accuracy: 0.7368\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 3s 688ms/step - loss: 0.4784 - accuracy: 0.7429 - val_loss: 0.4634 - val_accuracy: 0.7368\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 3s 689ms/step - loss: 0.4697 - accuracy: 0.7429 - val_loss: 0.4624 - val_accuracy: 0.7368\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 3s 692ms/step - loss: 0.4738 - accuracy: 0.7238 - val_loss: 0.4722 - val_accuracy: 0.6842\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 3s 732ms/step - loss: 0.4725 - accuracy: 0.7238 - val_loss: 0.4617 - val_accuracy: 0.7368\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 3s 720ms/step - loss: 0.4634 - accuracy: 0.7524 - val_loss: 0.4706 - val_accuracy: 0.6842\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 3s 686ms/step - loss: 0.4694 - accuracy: 0.7429 - val_loss: 0.4784 - val_accuracy: 0.6842\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 3s 707ms/step - loss: 0.4664 - accuracy: 0.7333 - val_loss: 0.4614 - val_accuracy: 0.6842\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 3s 688ms/step - loss: 0.4744 - accuracy: 0.7524 - val_loss: 0.4582 - val_accuracy: 0.7368\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 3s 712ms/step - loss: 0.4753 - accuracy: 0.7524 - val_loss: 0.4571 - val_accuracy: 0.7368\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 3s 706ms/step - loss: 0.4618 - accuracy: 0.7619 - val_loss: 0.4555 - val_accuracy: 0.7368\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 3s 697ms/step - loss: 0.4634 - accuracy: 0.7524 - val_loss: 0.4545 - val_accuracy: 0.7368\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 3s 696ms/step - loss: 0.4605 - accuracy: 0.7619 - val_loss: 0.4567 - val_accuracy: 0.7368\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 3s 691ms/step - loss: 0.4607 - accuracy: 0.7524 - val_loss: 0.4551 - val_accuracy: 0.7368\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 3s 683ms/step - loss: 0.4600 - accuracy: 0.7524 - val_loss: 0.4545 - val_accuracy: 0.7368\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 0.4608 - accuracy: 0.7619 - val_loss: 0.4554 - val_accuracy: 0.7368\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 3s 700ms/step - loss: 0.4565 - accuracy: 0.7619 - val_loss: 0.4567 - val_accuracy: 0.7895\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 0.4576 - accuracy: 0.7619 - val_loss: 0.4545 - val_accuracy: 0.7895\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 3s 691ms/step - loss: 0.4622 - accuracy: 0.7619 - val_loss: 0.4510 - val_accuracy: 0.7368\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 0.4559 - accuracy: 0.7524 - val_loss: 0.4629 - val_accuracy: 0.6842\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 3s 687ms/step - loss: 0.4552 - accuracy: 0.7524 - val_loss: 0.4634 - val_accuracy: 0.6842\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 3s 683ms/step - loss: 0.4541 - accuracy: 0.7524 - val_loss: 0.4557 - val_accuracy: 0.7895\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 3s 683ms/step - loss: 0.4503 - accuracy: 0.7619 - val_loss: 0.4509 - val_accuracy: 0.7368\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 3s 682ms/step - loss: 0.4603 - accuracy: 0.7619 - val_loss: 0.4483 - val_accuracy: 0.7368\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 3s 681ms/step - loss: 0.4490 - accuracy: 0.7619 - val_loss: 0.4568 - val_accuracy: 0.7895\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 3s 682ms/step - loss: 0.4558 - accuracy: 0.7714 - val_loss: 0.4495 - val_accuracy: 0.6842\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 3s 680ms/step - loss: 0.4593 - accuracy: 0.7619 - val_loss: 0.4544 - val_accuracy: 0.7368\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 3s 686ms/step - loss: 0.4679 - accuracy: 0.7619 - val_loss: 0.4552 - val_accuracy: 0.7895\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 3s 694ms/step - loss: 0.4574 - accuracy: 0.7714 - val_loss: 0.4520 - val_accuracy: 0.7895\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 3s 683ms/step - loss: 0.4410 - accuracy: 0.7714 - val_loss: 0.4595 - val_accuracy: 0.7368\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 3s 683ms/step - loss: 0.4724 - accuracy: 0.7905 - val_loss: 0.4491 - val_accuracy: 0.7368\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 3s 682ms/step - loss: 0.4442 - accuracy: 0.7619 - val_loss: 0.4695 - val_accuracy: 0.6842\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 3s 686ms/step - loss: 0.4580 - accuracy: 0.7524 - val_loss: 0.4942 - val_accuracy: 0.7368\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 3s 686ms/step - loss: 0.4554 - accuracy: 0.7714 - val_loss: 0.4506 - val_accuracy: 0.7368\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 3s 738ms/step - loss: 0.4505 - accuracy: 0.7619 - val_loss: 0.4585 - val_accuracy: 0.7368\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 3s 688ms/step - loss: 0.4510 - accuracy: 0.7810 - val_loss: 0.4571 - val_accuracy: 0.7895\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 3s 689ms/step - loss: 0.4389 - accuracy: 0.7810 - val_loss: 0.4602 - val_accuracy: 0.7895\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 3s 687ms/step - loss: 0.4304 - accuracy: 0.7905 - val_loss: 0.4593 - val_accuracy: 0.7368\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 3s 681ms/step - loss: 0.4414 - accuracy: 0.7714 - val_loss: 0.4696 - val_accuracy: 0.7368\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 0.4432 - accuracy: 0.7714 - val_loss: 0.4631 - val_accuracy: 0.7368\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 3s 683ms/step - loss: 0.4685 - accuracy: 0.7714 - val_loss: 0.4696 - val_accuracy: 0.7368\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 3s 681ms/step - loss: 0.4761 - accuracy: 0.7714 - val_loss: 0.4840 - val_accuracy: 0.6842\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 3s 682ms/step - loss: 0.4379 - accuracy: 0.7714 - val_loss: 0.4587 - val_accuracy: 0.7895\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 3s 688ms/step - loss: 0.4419 - accuracy: 0.7714 - val_loss: 0.4569 - val_accuracy: 0.7368\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 3s 683ms/step - loss: 0.4275 - accuracy: 0.8000 - val_loss: 0.4773 - val_accuracy: 0.6842\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 3s 699ms/step - loss: 0.4342 - accuracy: 0.7810 - val_loss: 0.5059 - val_accuracy: 0.6842\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 3s 723ms/step - loss: 0.4371 - accuracy: 0.7905 - val_loss: 0.4733 - val_accuracy: 0.7368\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 0.4226 - accuracy: 0.8190 - val_loss: 0.4681 - val_accuracy: 0.7895\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 3s 702ms/step - loss: 0.4209 - accuracy: 0.8095 - val_loss: 0.4824 - val_accuracy: 0.7368\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 3s 706ms/step - loss: 0.4204 - accuracy: 0.8095 - val_loss: 0.4711 - val_accuracy: 0.7895\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 3s 702ms/step - loss: 0.4221 - accuracy: 0.8190 - val_loss: 0.4657 - val_accuracy: 0.7368\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 3s 771ms/step - loss: 0.4175 - accuracy: 0.7905 - val_loss: 0.5083 - val_accuracy: 0.7368\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.4207 - accuracy: 0.7905 - val_loss: 0.4812 - val_accuracy: 0.7895\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.4053 - accuracy: 0.8095 - val_loss: 0.4848 - val_accuracy: 0.7895\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.4165 - accuracy: 0.8190 - val_loss: 0.4857 - val_accuracy: 0.7895\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.3949 - accuracy: 0.8190 - val_loss: 0.5173 - val_accuracy: 0.7368\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.4030 - accuracy: 0.7810 - val_loss: 0.4927 - val_accuracy: 0.7895\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.3987 - accuracy: 0.8190 - val_loss: 0.4934 - val_accuracy: 0.7368\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.3961 - accuracy: 0.8286 - val_loss: 0.5682 - val_accuracy: 0.7368\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 9s 2s/step - loss: 0.4609 - accuracy: 0.7429 - val_loss: 0.5593 - val_accuracy: 0.7368\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 9s 2s/step - loss: 0.4056 - accuracy: 0.8000 - val_loss: 0.4884 - val_accuracy: 0.6842\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.4287 - accuracy: 0.8190 - val_loss: 0.5190 - val_accuracy: 0.7368\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3983 - accuracy: 0.8000 - val_loss: 0.4824 - val_accuracy: 0.7368\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3990 - accuracy: 0.8381 - val_loss: 0.4841 - val_accuracy: 0.7368\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3965 - accuracy: 0.8000 - val_loss: 0.5304 - val_accuracy: 0.7368\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.4027 - accuracy: 0.8000 - val_loss: 0.5320 - val_accuracy: 0.7368\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3948 - accuracy: 0.7619 - val_loss: 0.4970 - val_accuracy: 0.7368\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3821 - accuracy: 0.8381 - val_loss: 0.5340 - val_accuracy: 0.7368\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.4259 - accuracy: 0.7714 - val_loss: 0.5681 - val_accuracy: 0.7368\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3863 - accuracy: 0.8095 - val_loss: 0.4905 - val_accuracy: 0.7368\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3903 - accuracy: 0.8286 - val_loss: 0.4903 - val_accuracy: 0.7368\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3980 - accuracy: 0.7714 - val_loss: 0.5071 - val_accuracy: 0.7368\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3820 - accuracy: 0.8476 - val_loss: 0.4950 - val_accuracy: 0.6842\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3826 - accuracy: 0.8381 - val_loss: 0.5026 - val_accuracy: 0.7895\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3722 - accuracy: 0.7905 - val_loss: 0.5130 - val_accuracy: 0.7895\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3778 - accuracy: 0.8095 - val_loss: 0.5104 - val_accuracy: 0.6842\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3678 - accuracy: 0.8000 - val_loss: 0.5298 - val_accuracy: 0.7368\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3765 - accuracy: 0.8190 - val_loss: 0.5242 - val_accuracy: 0.6842\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3705 - accuracy: 0.8190 - val_loss: 0.5882 - val_accuracy: 0.7368\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3880 - accuracy: 0.7810 - val_loss: 0.5207 - val_accuracy: 0.6842\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3412 - accuracy: 0.8286 - val_loss: 0.6526 - val_accuracy: 0.7368\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3910 - accuracy: 0.7905 - val_loss: 0.4813 - val_accuracy: 0.7895\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3587 - accuracy: 0.8571 - val_loss: 0.4985 - val_accuracy: 0.7368\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3533 - accuracy: 0.8000 - val_loss: 0.5699 - val_accuracy: 0.7368\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3466 - accuracy: 0.8190 - val_loss: 0.5276 - val_accuracy: 0.6842\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3405 - accuracy: 0.8571 - val_loss: 0.6339 - val_accuracy: 0.6842\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3317 - accuracy: 0.8095 - val_loss: 0.6101 - val_accuracy: 0.7368\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3333 - accuracy: 0.8571 - val_loss: 0.5824 - val_accuracy: 0.7368\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3541 - accuracy: 0.8190 - val_loss: 0.5285 - val_accuracy: 0.6842\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3214 - accuracy: 0.8571 - val_loss: 0.5187 - val_accuracy: 0.6842\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3113 - accuracy: 0.8571 - val_loss: 0.7356 - val_accuracy: 0.7368\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3213 - accuracy: 0.8381 - val_loss: 0.6307 - val_accuracy: 0.7895\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.4031 - accuracy: 0.8000 - val_loss: 0.9757 - val_accuracy: 0.6316\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.4995 - accuracy: 0.7524 - val_loss: 0.5060 - val_accuracy: 0.7895\n",
      "4/4 [==============================] - 2s 327ms/step - loss: 0.9171 - accuracy: 0.5938\n",
      "Test accuracy: 59.38%\n"
     ]
    }
   ],
   "source": [
    "trained_model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236a77d30b234d6fa87451df6ff90e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(VBox(children=(HTML(value=\"'T: 0 | P: 0'\"), Box(children=(Image(value=b'GIF87a \\x00 \\x00\\x87…"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_SAMPLES_VIZ = 4\n",
    "t = iter(testloader)\n",
    "for i in range(1):\n",
    "    testsamples, labels = t.get_next()\n",
    "testsamples, labels = testsamples[:NUM_SAMPLES_VIZ], labels[:NUM_SAMPLES_VIZ]\n",
    "\n",
    "ground_truths = []\n",
    "preds = []\n",
    "videos = []\n",
    "\n",
    "for i, (testsample, label) in enumerate(zip(testsamples, labels)):\n",
    "    # Generate gif\n",
    "    with io.BytesIO() as gif:\n",
    "        imageio.mimsave(gif, (testsample.numpy()*255).astype(\"uint8\"), \"GIF\")\n",
    "        videos.append(gif.getvalue())\n",
    "\n",
    "    # Get model prediction\n",
    "    output = trained_model.predict(tf.expand_dims(testsample, axis=0))[0]\n",
    "    pred = np.argmax(output, axis=0)\n",
    "\n",
    "    ground_truths.append(label.numpy().astype(\"int\"))\n",
    "    preds.append(pred)\n",
    "\n",
    "\n",
    "def make_box_for_grid(image_widget, fit):\n",
    "    \"\"\"Make a VBox to hold caption/image for demonstrating option_fit values.\n",
    "\n",
    "    Source: https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20Styling.html\n",
    "    \"\"\"\n",
    "    # Make the caption\n",
    "    if fit is not None:\n",
    "        fit_str = \"'{}'\".format(fit)\n",
    "    else:\n",
    "        fit_str = str(fit)\n",
    "\n",
    "    h = ipywidgets.HTML(value=\"\" + str(fit_str) + \"\")\n",
    "\n",
    "    # Make the green box with the image widget inside it\n",
    "    boxb = ipywidgets.widgets.Box()\n",
    "    boxb.children = [image_widget]\n",
    "\n",
    "    # Compose into a vertical box\n",
    "    vb = ipywidgets.widgets.VBox()\n",
    "    vb.layout.align_items = \"center\"\n",
    "    vb.children = [h, boxb]\n",
    "    return vb\n",
    "\n",
    "\n",
    "boxes = []\n",
    "#class_vocab = [\"Not Yawning\", \"Yawning\"]\n",
    "\n",
    "for i in range(NUM_SAMPLES_VIZ):\n",
    "    ib = ipywidgets.widgets.Image(value=videos[i], width=100, height=100)\n",
    "    true_class = str(ground_truths[i])\n",
    "    pred_class = str(preds[i])\n",
    "    caption = f\"T: {true_class} | P: {pred_class}\"\n",
    "\n",
    "    boxes.append(make_box_for_grid(ib, caption))\n",
    "\n",
    "ipywidgets.widgets.GridBox(\n",
    "    boxes, layout=ipywidgets.widgets.Layout(grid_template_columns=\"repeat(5, 200px)\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
