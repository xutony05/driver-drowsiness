{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skipping frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import imageio\n",
    "import medmnist\n",
    "import ipywidgets\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 128\n",
    "MAX_SEQ_LENGTH = 724\n",
    "frame_step = 2\n",
    "\n",
    "# DATA\n",
    "BATCH_SIZE = 6\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (MAX_SEQ_LENGTH, IMG_SIZE, IMG_SIZE, 3)\n",
    "NUM_CLASSES = 1\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 25\n",
    "\n",
    "# TUBELET EMBEDDING\n",
    "PATCH_SIZE = (200, 32, 32)\n",
    "#NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "# ViViT ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "PROJECTION_DIM = 128\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Female Participant Number</th>\n",
       "      <th>Action</th>\n",
       "      <th>video-name</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>1-FemaleNoGlasses-Normal.avi</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Talking</td>\n",
       "      <td>1-FemaleNoGlasses-Talking.avi</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Yawning</td>\n",
       "      <td>1-FemaleNoGlasses-Yawning.avi</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Normal</td>\n",
       "      <td>2-FemaleNoGlasses-Normal.avi</td>\n",
       "      <td>C:\\Users\\bnich\\Documents\\_MyDocs\\_School\\2022-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Talking</td>\n",
       "      <td>2-FemaleNoGlasses-Talking.avi</td>\n",
       "      <td>C:\\Users\\bnich\\Documents\\_MyDocs\\_School\\2022-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Female Participant Number   Action                     video-name  \\\n",
       "0                          1   Normal   1-FemaleNoGlasses-Normal.avi   \n",
       "1                          1  Talking  1-FemaleNoGlasses-Talking.avi   \n",
       "2                          1  Yawning  1-FemaleNoGlasses-Yawning.avi   \n",
       "3                          2   Normal   2-FemaleNoGlasses-Normal.avi   \n",
       "4                          2  Talking  2-FemaleNoGlasses-Talking.avi   \n",
       "\n",
       "                                          Unnamed: 3  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3  C:\\Users\\bnich\\Documents\\_MyDocs\\_School\\2022-...  \n",
       "4  C:\\Users\\bnich\\Documents\\_MyDocs\\_School\\2022-...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"../data/female-mirror-data.csv\")\n",
    "df1[\"Action\"] = df1[\"Action\"].str.rstrip()\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"target\"] = (df1[\"Action\"]==\"Yawning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "dfTrain = pd.DataFrame()\n",
    "dfTest = pd.DataFrame()\n",
    "\n",
    "while i<len(df1):\n",
    "    if i%5==0:\n",
    "        dfTest = pd.concat([dfTest, df1.iloc[[i]]])\n",
    "    else :\n",
    "        dfTrain = pd.concat([dfTrain, df1.iloc[[i]]])\n",
    "\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTest.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain[dfTrain.target == True].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTest[dfTest.target == True].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_frames(frame, output_size):\n",
    "  \"\"\"\n",
    "    Pad and resize an image from a video.\n",
    "\n",
    "    Args:\n",
    "      frame: Image that needs to resized and padded. \n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      Formatted frame with padding of specified output size.\n",
    "  \"\"\"\n",
    "  frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
    "  frame = tf.image.resize_with_pad(frame, *output_size)\n",
    "  return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_from_video_file(video_path, MAX_SEQ_LENGTH, frame_step, output_size = (IMG_SIZE, IMG_SIZE)):\n",
    "  \"\"\"\n",
    "    Creates frames from each video file present for each category.\n",
    "\n",
    "    Args:\n",
    "      video_path: File path to the video.\n",
    "      n_frames: Number of frames to be created per video file.\n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
    "  \"\"\"\n",
    "  # Read each video frame by frame\n",
    "  result = []\n",
    "  \n",
    "  src = cv2.VideoCapture(str(video_path))  \n",
    "\n",
    "  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "  need_length = 1 + (MAX_SEQ_LENGTH - 1) * frame_step\n",
    "\n",
    "  if need_length > video_length:\n",
    "    start = 0\n",
    "  else:\n",
    "    max_start = video_length - need_length\n",
    "    start = random.randint(0, max_start + 1)\n",
    "\n",
    "  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "  # ret is a boolean indicating whether read was successful, frame is the image itself\n",
    "  ret, frame = src.read()\n",
    "  result.append(format_frames(frame, output_size))\n",
    "\n",
    "  for _ in range(MAX_SEQ_LENGTH - 1):\n",
    "    for _ in range(frame_step):\n",
    "      ret, frame = src.read()\n",
    "    if ret:\n",
    "      frame = format_frames(frame, output_size)\n",
    "      result.append(frame)\n",
    "    else:\n",
    "      result.append(np.zeros_like(result[0]))\n",
    "  src.release()\n",
    "  result = np.array(result)[..., [2, 1, 0]]\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameGenerator:\n",
    "  def __init__(self, df, root_dir, MAX_SEQ_LENGTH, frame_step):\n",
    "    \"\"\" Returns a set of frames with their associated label. \n",
    "\n",
    "      Args:\n",
    "        path: Video file paths.\n",
    "        n_frames: Number of frames. \n",
    "        training: Boolean to determine if training dataset is being created.\n",
    "    \"\"\"\n",
    "    self.df = df\n",
    "    self.n_frames = MAX_SEQ_LENGTH\n",
    "    self.root_dir = root_dir\n",
    "    self.frame_step = frame_step\n",
    "\n",
    "  def __call__(self):\n",
    "    video_paths = self.df[\"video-name\"].values.tolist()\n",
    "    labels = self.df[\"target\"].values.tolist()\n",
    "\n",
    "    for idx, path in enumerate(video_paths):\n",
    "      video_frames = frames_from_video_file(os.path.join(self.root_dir, path), self.n_frames, frame_step) \n",
    "      yield video_frames, labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n",
    "                    tf.TensorSpec(shape = (), dtype = tf.bool))\n",
    "train_ds = tf.data.Dataset.from_generator(FrameGenerator(dfTrain, \"../data/YawDD/YawDD dataset/Mirror/Female_mirror/\", MAX_SEQ_LENGTH, frame_step),  output_signature = output_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.data.Dataset.from_generator(FrameGenerator(dfTest, \"../data/YawDD/YawDD dataset/Mirror/Female_mirror/\", MAX_SEQ_LENGTH, frame_step),  output_signature = output_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)\n",
    "test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.batch(BATCH_SIZE)\n",
    "test_ds = test_ds.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TubeletEmbedding(layers.Layer):\n",
    "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.projection = layers.Conv3D(\n",
    "            filters=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=patch_size,\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
    "\n",
    "    def call(self, videos):\n",
    "        projected_patches = self.projection(videos)\n",
    "        flattened_patches = self.flatten(projected_patches)\n",
    "        return flattened_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, num_tokens, _ = input_shape\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_tokens, output_dim=self.embed_dim\n",
    "        )\n",
    "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
    "\n",
    "    def call(self, encoded_tokens):\n",
    "        # Encode the positions and add it to the encoded tokens\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_tokens = encoded_tokens + encoded_positions\n",
    "        return encoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vivit_classifier(\n",
    "    tubelet_embedder,\n",
    "    positional_encoder,\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    transformer_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embed_dim=PROJECTION_DIM,\n",
    "    layer_norm_eps=LAYER_NORM_EPS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "):\n",
    "    # Get the input layer\n",
    "    inputs = layers.Input(shape=(input_shape))\n",
    "    # Create patches.\n",
    "    patches = tubelet_embedder(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = positional_encoder(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization and MHSA\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=embed_dim * 4, activation=tf.nn.gelu),\n",
    "                layers.Dense(units=embed_dim, activation=tf.nn.gelu),\n",
    "            ]\n",
    "        )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
    "    representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(units=num_classes, activation=\"sigmoid\")(representation)\n",
    "\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "    # Initialize model\n",
    "    model = create_vivit_classifier(\n",
    "        tubelet_embedder=TubeletEmbedding(\n",
    "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
    "        ),\n",
    "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
    "    )\n",
    "\n",
    "    # Compile the model with the optimizer, loss function\n",
    "    # and the metrics.\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\n",
    "            \"accuracy\"\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Train the model.\n",
    "    _ = model.fit(train_ds, epochs=EPOCHS, validation_data = test_ds)\n",
    "\n",
    "    _, accuracy = model.evaluate(test_ds)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    #print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "21/21 [==============================] - 1123s 21s/step - loss: 0.6847 - accuracy: 0.6371 - val_loss: 0.5765 - val_accuracy: 0.6875\n",
      "Epoch 2/25\n",
      "21/21 [==============================] - 95s 5s/step - loss: 0.5732 - accuracy: 0.7016 - val_loss: 0.6201 - val_accuracy: 0.7188\n",
      "Epoch 3/25\n",
      "21/21 [==============================] - 130s 6s/step - loss: 0.5477 - accuracy: 0.6935 - val_loss: 0.6007 - val_accuracy: 0.7188\n",
      "Epoch 4/25\n",
      "21/21 [==============================] - 157s 8s/step - loss: 0.5311 - accuracy: 0.6855 - val_loss: 0.6035 - val_accuracy: 0.7188\n",
      "Epoch 5/25\n",
      "21/21 [==============================] - 96s 5s/step - loss: 0.5154 - accuracy: 0.7016 - val_loss: 0.6687 - val_accuracy: 0.7188\n",
      "Epoch 6/25\n",
      "21/21 [==============================] - 104s 5s/step - loss: 0.5082 - accuracy: 0.7016 - val_loss: 0.6922 - val_accuracy: 0.7188\n",
      "Epoch 7/25\n",
      "21/21 [==============================] - 123s 6s/step - loss: 0.4933 - accuracy: 0.7823 - val_loss: 0.6255 - val_accuracy: 0.7188\n",
      "Epoch 8/25\n",
      "21/21 [==============================] - 111s 5s/step - loss: 0.4979 - accuracy: 0.7339 - val_loss: 0.6926 - val_accuracy: 0.7188\n",
      "Epoch 9/25\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.5056 - accuracy: 0.7258 - val_loss: 0.7452 - val_accuracy: 0.6250\n",
      "Epoch 10/25\n",
      "21/21 [==============================] - 104s 5s/step - loss: 0.5133 - accuracy: 0.6855 - val_loss: 0.6280 - val_accuracy: 0.7188\n",
      "Epoch 11/25\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.4816 - accuracy: 0.7258 - val_loss: 0.6785 - val_accuracy: 0.6562\n",
      "Epoch 12/25\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.4748 - accuracy: 0.7258 - val_loss: 0.7006 - val_accuracy: 0.6562\n",
      "Epoch 13/25\n",
      "21/21 [==============================] - 67s 3s/step - loss: 0.4698 - accuracy: 0.7339 - val_loss: 0.6847 - val_accuracy: 0.6562\n",
      "Epoch 14/25\n",
      "21/21 [==============================] - 71s 3s/step - loss: 0.4640 - accuracy: 0.7258 - val_loss: 0.7112 - val_accuracy: 0.6562\n",
      "Epoch 15/25\n",
      "21/21 [==============================] - 78s 4s/step - loss: 0.4616 - accuracy: 0.7258 - val_loss: 0.7312 - val_accuracy: 0.6562\n",
      "Epoch 16/25\n",
      "21/21 [==============================] - 156s 8s/step - loss: 0.4614 - accuracy: 0.7419 - val_loss: 0.7707 - val_accuracy: 0.6875\n",
      "Epoch 17/25\n",
      "21/21 [==============================] - 164s 8s/step - loss: 0.4684 - accuracy: 0.7177 - val_loss: 0.7461 - val_accuracy: 0.6562\n",
      "Epoch 18/25\n",
      "21/21 [==============================] - 145s 7s/step - loss: 0.4510 - accuracy: 0.7581 - val_loss: 0.7319 - val_accuracy: 0.6562\n",
      "Epoch 19/25\n",
      "21/21 [==============================] - 109s 5s/step - loss: 0.4545 - accuracy: 0.7581 - val_loss: 0.8197 - val_accuracy: 0.6250\n",
      "Epoch 20/25\n",
      "21/21 [==============================] - 103s 5s/step - loss: 0.4534 - accuracy: 0.7500 - val_loss: 0.7575 - val_accuracy: 0.6562\n",
      "Epoch 21/25\n",
      "21/21 [==============================] - 94s 4s/step - loss: 0.4493 - accuracy: 0.7661 - val_loss: 0.7357 - val_accuracy: 0.7188\n",
      "Epoch 22/25\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.4543 - accuracy: 0.7339 - val_loss: 0.7820 - val_accuracy: 0.6562\n",
      "Epoch 23/25\n",
      "21/21 [==============================] - 80s 4s/step - loss: 0.4544 - accuracy: 0.7177 - val_loss: 0.7904 - val_accuracy: 0.6562\n",
      "Epoch 24/25\n",
      "21/21 [==============================] - 93s 4s/step - loss: 0.4481 - accuracy: 0.7500 - val_loss: 0.7392 - val_accuracy: 0.7188\n",
      "Epoch 25/25\n",
      "21/21 [==============================] - 95s 5s/step - loss: 0.4499 - accuracy: 0.7500 - val_loss: 0.7864 - val_accuracy: 0.6562\n",
      "6/6 [==============================] - 13s 2s/step - loss: 0.7864 - accuracy: 0.6562\n",
      "Test accuracy: 65.62%\n"
     ]
    }
   ],
   "source": [
    "trained_model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ee84029cfe4b30afde27812400d588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(VBox(children=(HTML(value=\"'T: 0 | P: 0'\"), Box(children=(Image(value=b'GIF89a\\x80\\x00\\x80\\xâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_SAMPLES_VIZ = 4\n",
    "t = iter(test_ds)\n",
    "for i in range(1):\n",
    "    testsamples, labels = t.get_next()\n",
    "testsamples, labels = testsamples[:NUM_SAMPLES_VIZ], labels[:NUM_SAMPLES_VIZ]\n",
    "\n",
    "ground_truths = []\n",
    "preds = []\n",
    "videos = []\n",
    "\n",
    "for i, (testsample, label) in enumerate(zip(testsamples, labels)):\n",
    "    # Generate gif\n",
    "    with io.BytesIO() as gif:\n",
    "        imageio.mimsave(gif, (testsample.numpy()*255).astype(\"uint8\"), \"GIF\")\n",
    "        videos.append(gif.getvalue())\n",
    "\n",
    "    # Get model prediction\n",
    "    output = trained_model.predict(tf.expand_dims(testsample, axis=0))[0]\n",
    "    pred = np.argmax(output, axis=0)\n",
    "\n",
    "    ground_truths.append(label.numpy().astype(\"int\"))\n",
    "    preds.append(pred)\n",
    "\n",
    "\n",
    "def make_box_for_grid(image_widget, fit):\n",
    "    \"\"\"Make a VBox to hold caption/image for demonstrating option_fit values.\n",
    "\n",
    "    Source: https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20Styling.html\n",
    "    \"\"\"\n",
    "    # Make the caption\n",
    "    if fit is not None:\n",
    "        fit_str = \"'{}'\".format(fit)\n",
    "    else:\n",
    "        fit_str = str(fit)\n",
    "\n",
    "    h = ipywidgets.HTML(value=\"\" + str(fit_str) + \"\")\n",
    "\n",
    "    # Make the green box with the image widget inside it\n",
    "    boxb = ipywidgets.widgets.Box()\n",
    "    boxb.children = [image_widget]\n",
    "\n",
    "    # Compose into a vertical box\n",
    "    vb = ipywidgets.widgets.VBox()\n",
    "    vb.layout.align_items = \"center\"\n",
    "    vb.children = [h, boxb]\n",
    "    return vb\n",
    "\n",
    "\n",
    "boxes = []\n",
    "#class_vocab = [\"Not Yawning\", \"Yawning\"]\n",
    "\n",
    "for i in range(NUM_SAMPLES_VIZ):\n",
    "    ib = ipywidgets.widgets.Image(value=videos[i], width=100, height=100)\n",
    "    true_class = str(ground_truths[i])\n",
    "    pred_class = str(preds[i])\n",
    "    caption = f\"T: {true_class} | P: {pred_class}\"\n",
    "\n",
    "    boxes.append(make_box_for_grid(ib, caption))\n",
    "\n",
    "ipywidgets.widgets.GridBox(\n",
    "    boxes, layout=ipywidgets.widgets.Layout(grid_template_columns=\"repeat(5, 200px)\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
