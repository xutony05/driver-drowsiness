{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import imageio\n",
    "import medmnist\n",
    "import ipywidgets\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"organmnist3d\"\n",
    "BATCH_SIZE = 32\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (28, 28, 28, 1)\n",
    "NUM_CLASSES = 11\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 20\n",
    "\n",
    "# TUBELET EMBEDDING\n",
    "PATCH_SIZE = (8, 8, 8)\n",
    "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "# ViViT ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "PROJECTION_DIM = 128\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_prepare_dataset(data_info: dict):\n",
    "    \"\"\"Utility function to download the dataset.\n",
    "\n",
    "    Arguments:\n",
    "        data_info (dict): Dataset metadata.\n",
    "    \"\"\"\n",
    "    data_path = keras.utils.get_file(\"OrganMNIST3D\", origin=data_info[\"url\"], md5_hash=data_info[\"MD5\"])\n",
    "\n",
    "    with np.load(data_path) as data:\n",
    "        # Get videos\n",
    "        train_videos = data[\"train_images\"]\n",
    "        valid_videos = data[\"val_images\"]\n",
    "        test_videos = data[\"test_images\"]\n",
    "\n",
    "        # Get labels\n",
    "        train_labels = data[\"train_labels\"].flatten()\n",
    "        valid_labels = data[\"val_labels\"].flatten()\n",
    "        test_labels = data[\"test_labels\"].flatten()\n",
    "\n",
    "    return (\n",
    "        (train_videos, train_labels),\n",
    "        (valid_videos, valid_labels),\n",
    "        (test_videos, test_labels),\n",
    "    )\n",
    "\n",
    "\n",
    "# Get the metadata of the dataset\n",
    "info = medmnist.INFO[DATASET_NAME]\n",
    "\n",
    "# Get the dataset\n",
    "prepared_dataset = download_and_prepare_dataset(info)\n",
    "(train_videos, train_labels) = prepared_dataset[0]\n",
    "(valid_videos, valid_labels) = prepared_dataset[1]\n",
    "(test_videos, test_labels) = prepared_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(972, 28, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_videos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(972,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def preprocess(frames: tf.Tensor, label: tf.Tensor):\n",
    "    \"\"\"Preprocess the frames tensors and parse the labels.\"\"\"\n",
    "    # Preprocess images\n",
    "    frames = tf.image.convert_image_dtype(\n",
    "        frames[\n",
    "            ..., tf.newaxis\n",
    "        ],  # The new axis is to help for further processing with Conv3D layers\n",
    "        tf.float32,\n",
    "    )\n",
    "    # Parse label\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return frames, label\n",
    "\n",
    "\n",
    "def prepare_dataloader(\n",
    "    videos: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    loader_type: str = \"train\",\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "):\n",
    "    \"\"\"Utility function to prepare the dataloader.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n",
    "\n",
    "    if loader_type == \"train\":\n",
    "        dataset = dataset.shuffle(BATCH_SIZE * 2)\n",
    "\n",
    "    dataloader = (\n",
    "        dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "trainloader = prepare_dataloader(train_videos, train_labels, \"train\")\n",
    "validloader = prepare_dataloader(valid_videos, valid_labels, \"valid\")\n",
    "testloader = prepare_dataloader(test_videos, test_labels, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, 28, 28, 28, 1), (None,)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TubeletEmbedding(layers.Layer):\n",
    "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.projection = layers.Conv3D(\n",
    "            filters=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=patch_size,\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
    "\n",
    "    def call(self, videos):\n",
    "        projected_patches = self.projection(videos)\n",
    "        flattened_patches = self.flatten(projected_patches)\n",
    "        return flattened_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, num_tokens, _ = input_shape\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_tokens, output_dim=self.embed_dim\n",
    "        )\n",
    "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
    "\n",
    "    def call(self, encoded_tokens):\n",
    "        # Encode the positions and add it to the encoded tokens\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_tokens = encoded_tokens + encoded_positions\n",
    "        return encoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vivit_classifier(\n",
    "    tubelet_embedder,\n",
    "    positional_encoder,\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    transformer_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embed_dim=PROJECTION_DIM,\n",
    "    layer_norm_eps=LAYER_NORM_EPS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "):\n",
    "    # Get the input layer\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Create patches.\n",
    "    patches = tubelet_embedder(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = positional_encoder(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization and MHSA\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=embed_dim * 4, activation=tf.nn.gelu),\n",
    "                layers.Dense(units=embed_dim, activation=tf.nn.gelu),\n",
    "            ]\n",
    "        )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
    "    representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(units=num_classes, activation=\"softmax\")(representation)\n",
    "\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "31/31 [==============================] - 21s 354ms/step - loss: 2.6404 - accuracy: 0.1099 - top-5-accuracy: 0.5480 - val_loss: 2.3779 - val_accuracy: 0.1801 - val_top-5-accuracy: 0.5466\n",
      "Epoch 2/20\n",
      "31/31 [==============================] - 13s 432ms/step - loss: 2.2484 - accuracy: 0.1828 - top-5-accuracy: 0.6626 - val_loss: 2.0904 - val_accuracy: 0.2236 - val_top-5-accuracy: 0.6460\n",
      "Epoch 3/20\n",
      "31/31 [==============================] - 13s 415ms/step - loss: 2.1504 - accuracy: 0.2169 - top-5-accuracy: 0.7000 - val_loss: 1.8555 - val_accuracy: 0.2919 - val_top-5-accuracy: 0.8447\n",
      "Epoch 4/20\n",
      "31/31 [==============================] - 12s 401ms/step - loss: 2.0482 - accuracy: 0.2272 - top-5-accuracy: 0.7755 - val_loss: 1.6612 - val_accuracy: 0.2857 - val_top-5-accuracy: 0.9068\n",
      "Epoch 5/20\n",
      "31/31 [==============================] - 13s 424ms/step - loss: 1.8002 - accuracy: 0.2938 - top-5-accuracy: 0.8189 - val_loss: 1.4473 - val_accuracy: 0.2919 - val_top-5-accuracy: 0.9441\n",
      "Epoch 6/20\n",
      "31/31 [==============================] - 13s 410ms/step - loss: 1.5555 - accuracy: 0.4220 - top-5-accuracy: 0.8898 - val_loss: 1.2198 - val_accuracy: 0.4596 - val_top-5-accuracy: 0.9814\n",
      "Epoch 7/20\n",
      "31/31 [==============================] - 14s 441ms/step - loss: 1.4069 - accuracy: 0.4606 - top-5-accuracy: 0.9378 - val_loss: 1.0674 - val_accuracy: 0.5466 - val_top-5-accuracy: 0.9689\n",
      "Epoch 8/20\n",
      "31/31 [==============================] - 16s 514ms/step - loss: 1.3145 - accuracy: 0.5120 - top-5-accuracy: 0.9334 - val_loss: 1.2110 - val_accuracy: 0.5528 - val_top-5-accuracy: 0.9627\n",
      "Epoch 9/20\n",
      "31/31 [==============================] - 14s 461ms/step - loss: 1.3712 - accuracy: 0.4757 - top-5-accuracy: 0.9459 - val_loss: 1.3292 - val_accuracy: 0.4845 - val_top-5-accuracy: 0.9627\n",
      "Epoch 10/20\n",
      "31/31 [==============================] - 13s 409ms/step - loss: 1.2949 - accuracy: 0.5162 - top-5-accuracy: 0.9325 - val_loss: 0.9689 - val_accuracy: 0.5963 - val_top-5-accuracy: 0.9876\n",
      "Epoch 11/20\n",
      "31/31 [==============================] - 13s 429ms/step - loss: 1.1313 - accuracy: 0.5612 - top-5-accuracy: 0.9603 - val_loss: 0.7554 - val_accuracy: 0.6832 - val_top-5-accuracy: 0.9938\n",
      "Epoch 12/20\n",
      "31/31 [==============================] - 13s 417ms/step - loss: 0.9777 - accuracy: 0.6164 - top-5-accuracy: 0.9699 - val_loss: 0.6876 - val_accuracy: 0.7764 - val_top-5-accuracy: 0.9876\n",
      "Epoch 13/20\n",
      "31/31 [==============================] - 9s 293ms/step - loss: 0.8817 - accuracy: 0.6573 - top-5-accuracy: 0.9760 - val_loss: 0.6632 - val_accuracy: 0.7764 - val_top-5-accuracy: 0.9876\n",
      "Epoch 14/20\n",
      "31/31 [==============================] - 7s 232ms/step - loss: 0.9215 - accuracy: 0.6411 - top-5-accuracy: 0.9770 - val_loss: 0.7495 - val_accuracy: 0.7019 - val_top-5-accuracy: 0.9876\n",
      "Epoch 15/20\n",
      "31/31 [==============================] - 8s 260ms/step - loss: 0.8206 - accuracy: 0.6902 - top-5-accuracy: 0.9851 - val_loss: 0.4590 - val_accuracy: 0.8696 - val_top-5-accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "31/31 [==============================] - 8s 257ms/step - loss: 0.6529 - accuracy: 0.7525 - top-5-accuracy: 0.9869 - val_loss: 0.5052 - val_accuracy: 0.8323 - val_top-5-accuracy: 0.9938\n",
      "Epoch 17/20\n",
      "31/31 [==============================] - 8s 261ms/step - loss: 0.7376 - accuracy: 0.7326 - top-5-accuracy: 0.9833 - val_loss: 0.5383 - val_accuracy: 0.8199 - val_top-5-accuracy: 0.9814\n",
      "Epoch 18/20\n",
      "31/31 [==============================] - 8s 263ms/step - loss: 0.5710 - accuracy: 0.7913 - top-5-accuracy: 0.9922 - val_loss: 0.4688 - val_accuracy: 0.8571 - val_top-5-accuracy: 0.9938\n",
      "Epoch 19/20\n",
      "31/31 [==============================] - 9s 276ms/step - loss: 0.5783 - accuracy: 0.7541 - top-5-accuracy: 0.9954 - val_loss: 0.4398 - val_accuracy: 0.8820 - val_top-5-accuracy: 0.9938\n",
      "Epoch 20/20\n",
      "31/31 [==============================] - 9s 283ms/step - loss: 0.5485 - accuracy: 0.7871 - top-5-accuracy: 0.9955 - val_loss: 0.4742 - val_accuracy: 0.8696 - val_top-5-accuracy: 0.9876\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 1.0191 - accuracy: 0.6623 - top-5-accuracy: 0.9836\n",
      "Test accuracy: 66.23%\n",
      "Test top 5 accuracy: 98.36%\n"
     ]
    }
   ],
   "source": [
    "def run_experiment():\n",
    "    # Initialize model\n",
    "    model = create_vivit_classifier(\n",
    "        tubelet_embedder=TubeletEmbedding(\n",
    "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
    "        ),\n",
    "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
    "    )\n",
    "\n",
    "    # Compile the model with the optimizer, loss function\n",
    "    # and the metrics.\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Train the model.\n",
    "    _ = model.fit(trainloader, epochs=EPOCHS, validation_data=validloader)\n",
    "\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(testloader)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7e42d4079545809f1e36d7303897d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(VBox(children=(HTML(value=\"'T: pancreas | P: bladder'\"), Box(children=(Image(value=b'GIF89a\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_SAMPLES_VIZ = 25\n",
    "testsamples, labels = next(iter(testloader))\n",
    "testsamples, labels = testsamples[:NUM_SAMPLES_VIZ], labels[:NUM_SAMPLES_VIZ]\n",
    "\n",
    "ground_truths = []\n",
    "preds = []\n",
    "videos = []\n",
    "\n",
    "for i, (testsample, label) in enumerate(zip(testsamples, labels)):\n",
    "    # Generate gif\n",
    "    with io.BytesIO() as gif:\n",
    "        imageio.mimsave(gif, (testsample.numpy() * 255).astype(\"uint8\"), \"GIF\", fps=5)\n",
    "        videos.append(gif.getvalue())\n",
    "\n",
    "    # Get model prediction\n",
    "    output = model.predict(tf.expand_dims(testsample, axis=0))[0]\n",
    "    pred = np.argmax(output, axis=0)\n",
    "\n",
    "    ground_truths.append(label.numpy().astype(\"int\"))\n",
    "    preds.append(pred)\n",
    "\n",
    "\n",
    "def make_box_for_grid(image_widget, fit):\n",
    "    \"\"\"Make a VBox to hold caption/image for demonstrating option_fit values.\n",
    "\n",
    "    Source: https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20Styling.html\n",
    "    \"\"\"\n",
    "    # Make the caption\n",
    "    if fit is not None:\n",
    "        fit_str = \"'{}'\".format(fit)\n",
    "    else:\n",
    "        fit_str = str(fit)\n",
    "\n",
    "    h = ipywidgets.HTML(value=\"\" + str(fit_str) + \"\")\n",
    "\n",
    "    # Make the green box with the image widget inside it\n",
    "    boxb = ipywidgets.widgets.Box()\n",
    "    boxb.children = [image_widget]\n",
    "\n",
    "    # Compose into a vertical box\n",
    "    vb = ipywidgets.widgets.VBox()\n",
    "    vb.layout.align_items = \"center\"\n",
    "    vb.children = [h, boxb]\n",
    "    return vb\n",
    "\n",
    "\n",
    "boxes = []\n",
    "for i in range(NUM_SAMPLES_VIZ):\n",
    "    ib = ipywidgets.widgets.Image(value=videos[i], width=100, height=100)\n",
    "    true_class = info[\"label\"][str(ground_truths[i])]\n",
    "    pred_class = info[\"label\"][str(preds[i])]\n",
    "    caption = f\"T: {true_class} | P: {pred_class}\"\n",
    "\n",
    "    boxes.append(make_box_for_grid(ib, caption))\n",
    "\n",
    "ipywidgets.widgets.GridBox(\n",
    "    boxes, layout=ipywidgets.widgets.Layout(grid_template_columns=\"repeat(5, 200px)\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
