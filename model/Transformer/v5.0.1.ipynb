{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to follow this tutorial\n",
    "https://www.ai-contentlab.com/2023/03/how-to-implement-timesformer-for-video.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TimeSformer.timesformer.models.vit import TimeSformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TimeSformer(img_size=224, num_classes=3, num_frames=8, attention_type='divided_space_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_from_video_file(video_path, MAX_SEQ_LENGTH, frame_step, output_size = (IMG_SIZE, IMG_SIZE)):\n",
    "    result = []\n",
    "  \n",
    "    src = cv2.VideoCapture(str(video_path))  \n",
    "\n",
    "    video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "    need_length = 1 + (MAX_SEQ_LENGTH - 1) * frame_step\n",
    "\n",
    "    if need_length > video_length:\n",
    "        start = 0\n",
    "    else:\n",
    "        max_start = video_length - need_length\n",
    "        start = random.randint(0, max_start + 1)\n",
    "\n",
    "    src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "    # ret is a boolean indicating whether read was successful, frame is the image itself\n",
    "    ret, frame = src.read()\n",
    "    frame = cv2.resize(frame, output_size)\n",
    "    result.append(frame)\n",
    "\n",
    "    for _ in range(MAX_SEQ_LENGTH - 1):\n",
    "        for _ in range(frame_step):\n",
    "            ret, frame = src.read()\n",
    "        if ret:\n",
    "            frame = cv2.resize(frame, output_size)\n",
    "            result.append(frame)\n",
    "        else:\n",
    "            frame = np.zeros_like(result[0])\n",
    "            result.append(frame)\n",
    "    src.release()\n",
    "    result = np.array(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, root_dir, MAX_SEQ_LENGTH, frame_step):\n",
    "        self.video_paths = df[\"video-name\"].values.tolist()\n",
    "        self.labels = df[\"label\"].values.tolist()\n",
    "        self.n_frames = MAX_SEQ_LENGTH\n",
    "        self.root_dir = root_dir\n",
    "        self.frame_step = frame_step\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        frames = frames_from_video_file(os.path.join(self.root_dir, path), self.n_frames, self.frame_step)\n",
    "        frames = np.float32(frames)\n",
    "        frames = np.moveaxis(frames, -1, 0)\n",
    "        #print(frames.shape)\n",
    "        return frames, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/mirror-data.csv\")\n",
    "df[\"Action\"] = df[\"Action\"].str.rstrip()\n",
    "df = df[df.Action != \"Talking&Yawning\"]\n",
    "df[\"label\"] = df.Action.astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "dfTrain = pd.DataFrame()\n",
    "dfTest = pd.DataFrame()\n",
    "\n",
    "while i<len(df):\n",
    "    if i%5==0:\n",
    "        dfTest = pd.concat([dfTest, df.iloc[[i]]])\n",
    "    else :\n",
    "        dfTrain = pd.concat([dfTrain, df.iloc[[i]]])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 8\n",
    "frame_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = VideoDataset(dfTrain, \"../data/YawDD/YawDD dataset/Mirror/all/\", MAX_SEQ_LENGTH, frame_step)\n",
    "test_ds = VideoDataset(dfTest, \"../data/YawDD/YawDD dataset/Mirror/all/\", MAX_SEQ_LENGTH, frame_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[[255., 255., 255., ..., 255., 255., 255.],\n",
       "          [255., 255., 255., ..., 255., 255., 255.],\n",
       "          [255., 255., 255., ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [180., 184., 173., ..., 252., 255., 254.],\n",
       "          [173., 180., 173., ..., 252., 251., 250.],\n",
       "          [169., 178., 173., ..., 253., 251., 250.]],\n",
       " \n",
       "         [[255., 255., 255., ..., 255., 255., 255.],\n",
       "          [255., 255., 255., ..., 255., 255., 255.],\n",
       "          [255., 255., 255., ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [180., 181., 176., ..., 252., 255., 251.],\n",
       "          [173., 178., 175., ..., 252., 252., 252.],\n",
       "          [169., 176., 175., ..., 249., 251., 250.]],\n",
       " \n",
       "         [[255., 255., 255., ..., 255., 255., 255.],\n",
       "          [255., 255., 255., ..., 255., 255., 255.],\n",
       "          [255., 255., 255., ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [180., 181., 176., ..., 254., 253., 255.],\n",
       "          [173., 178., 175., ..., 254., 251., 252.],\n",
       "          [169., 176., 175., ..., 254., 255., 249.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[255., 255., 255., ..., 254., 254., 252.],\n",
       "          [255., 255., 255., ..., 255., 255., 252.],\n",
       "          [255., 255., 255., ..., 252., 252., 253.],\n",
       "          ...,\n",
       "          [183., 182., 184., ..., 255., 253., 252.],\n",
       "          [176., 176., 181., ..., 254., 254., 251.],\n",
       "          [171., 172., 180., ..., 254., 253., 255.]],\n",
       " \n",
       "         [[255., 255., 255., ..., 255., 254., 254.],\n",
       "          [255., 255., 255., ..., 255., 253., 253.],\n",
       "          [255., 255., 255., ..., 252., 255., 255.],\n",
       "          ...,\n",
       "          [182., 181., 186., ..., 255., 255., 254.],\n",
       "          [175., 175., 182., ..., 255., 253., 255.],\n",
       "          [171., 172., 181., ..., 255., 253., 255.]],\n",
       " \n",
       "         [[255., 255., 255., ..., 254., 254., 254.],\n",
       "          [255., 255., 255., ..., 253., 253., 253.],\n",
       "          [255., 255., 255., ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [182., 181., 186., ..., 254., 255., 252.],\n",
       "          [175., 175., 182., ..., 254., 253., 255.],\n",
       "          [171., 172., 181., ..., 254., 253., 255.]]],\n",
       " \n",
       " \n",
       "        [[[255., 255., 255., ..., 255., 255., 255.],\n",
       "          [255., 255., 255., ..., 255., 255., 255.],\n",
       "          [255., 255., 255., ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [166., 171., 162., ..., 251., 255., 253.],\n",
       "          [159., 167., 162., ..., 252., 250., 248.],\n",
       "          [155., 165., 162., ..., 253., 249., 247.]],\n",
       " \n",
       "         [[255., 255., 255., ..., 255., 255., 255.],\n",
       "          [255., 255., 255., ..., 255., 255., 255.],\n",
       "          [255., 255., 255., ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [166., 169., 166., ..., 252., 255., 251.],\n",
       "          [159., 165., 164., ..., 251., 250., 249.],\n",
       "          [155., 164., 164., ..., 248., 248., 246.]],\n",
       " \n",
       "         [[255., 255., 255., ..., 255., 255., 255.],\n",
       "          [255., 255., 255., ..., 255., 255., 255.],\n",
       "          [255., 255., 255., ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [166., 169., 166., ..., 253., 251., 254.],\n",
       "          [159., 165., 164., ..., 252., 249., 250.],\n",
       "          [155., 164., 164., ..., 252., 253., 247.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[255., 255., 255., ..., 254., 254., 252.],\n",
       "          [255., 255., 255., ..., 255., 255., 252.],\n",
       "          [255., 255., 255., ..., 252., 252., 253.],\n",
       "          ...,\n",
       "          [169., 168., 172., ..., 254., 252., 250.],\n",
       "          [162., 162., 169., ..., 253., 254., 249.],\n",
       "          [157., 158., 168., ..., 253., 252., 255.]],\n",
       " \n",
       "         [[255., 255., 255., ..., 255., 254., 254.],\n",
       "          [255., 255., 255., ..., 255., 253., 253.],\n",
       "          [255., 255., 255., ..., 252., 255., 255.],\n",
       "          ...,\n",
       "          [167., 167., 171., ..., 254., 255., 252.],\n",
       "          [161., 161., 167., ..., 254., 252., 253.],\n",
       "          [157., 158., 166., ..., 254., 252., 254.]],\n",
       " \n",
       "         [[255., 255., 255., ..., 254., 254., 254.],\n",
       "          [255., 255., 255., ..., 253., 253., 253.],\n",
       "          [255., 255., 255., ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [170., 169., 171., ..., 253., 253., 250.],\n",
       "          [163., 162., 167., ..., 253., 252., 254.],\n",
       "          [159., 159., 166., ..., 253., 252., 254.]]],\n",
       " \n",
       " \n",
       "        [[[255., 255., 255., ..., 255., 255., 255.],\n",
       "          [255., 255., 255., ..., 255., 255., 255.],\n",
       "          [255., 255., 255., ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [150., 159., 151., ..., 253., 255., 254.],\n",
       "          [144., 155., 151., ..., 253., 252., 253.],\n",
       "          [141., 153., 151., ..., 254., 252., 255.]],\n",
       " \n",
       "         [[255., 255., 255., ..., 255., 255., 255.],\n",
       "          [255., 255., 255., ..., 255., 255., 255.],\n",
       "          [255., 255., 255., ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [150., 155., 155., ..., 253., 255., 253.],\n",
       "          [144., 151., 153., ..., 253., 254., 255.],\n",
       "          [141., 150., 153., ..., 250., 254., 255.]],\n",
       " \n",
       "         [[255., 255., 255., ..., 255., 255., 255.],\n",
       "          [255., 255., 255., ..., 255., 255., 255.],\n",
       "          [255., 255., 255., ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [150., 155., 155., ..., 254., 255., 255.],\n",
       "          [144., 151., 153., ..., 254., 254., 254.],\n",
       "          [141., 150., 153., ..., 255., 255., 252.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[255., 255., 255., ..., 254., 254., 252.],\n",
       "          [255., 255., 255., ..., 255., 255., 252.],\n",
       "          [255., 255., 255., ..., 252., 252., 253.],\n",
       "          ...,\n",
       "          [153., 153., 158., ..., 255., 254., 255.],\n",
       "          [147., 147., 155., ..., 254., 254., 253.],\n",
       "          [142., 143., 154., ..., 254., 254., 255.]],\n",
       " \n",
       "         [[255., 255., 255., ..., 255., 254., 254.],\n",
       "          [255., 255., 255., ..., 255., 253., 253.],\n",
       "          [255., 255., 255., ..., 252., 255., 255.],\n",
       "          ...,\n",
       "          [153., 152., 158., ..., 255., 255., 255.],\n",
       "          [146., 146., 154., ..., 255., 254., 255.],\n",
       "          [142., 143., 153., ..., 255., 254., 255.]],\n",
       " \n",
       "         [[255., 255., 255., ..., 254., 254., 254.],\n",
       "          [255., 255., 255., ..., 253., 253., 253.],\n",
       "          [255., 255., 255., ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [150., 150., 158., ..., 254., 255., 255.],\n",
       "          [143., 144., 154., ..., 254., 254., 255.],\n",
       "          [139., 140., 153., ..., 254., 254., 255.]]]], dtype=float32),\n",
       " 0)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=4)\n",
    "val_loader = DataLoader(test_ds, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, l = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 8, 224, 224])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19544\\2086794504.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\xuton\\anaconda3\\envs\\timesformer\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[0;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         )\n\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\xuton\\anaconda3\\envs\\timesformer\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m def grad(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # Evaluate on validation set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_acc = 100 * correct / total\n",
    "    \n",
    "    print('[%d] loss: %.3f, val_acc: %.3f' %\n",
    "          (epoch + 1, running_loss / len(train_loader), val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timesformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
