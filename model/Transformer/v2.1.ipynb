{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on YawDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "train_ds = load_dataset('../data/archive/train/mouth', split='train')\n",
    "val_ds = load_dataset('../data/archive/val/mouth', split='train')\n",
    "test_ds = load_dataset('../data/test/mouth', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds['image'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds['label'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"Awake\", 1: \"Yawning\"}\n",
    "label2id = {label:id for id,label in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label[train_ds['label'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "image_mean = processor.image_mean\n",
    "image_std = processor.image_std\n",
    "size = processor.size[\"height\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (CenterCrop, \n",
    "                                    Compose, \n",
    "                                    Normalize, \n",
    "                                    RandomHorizontalFlip,\n",
    "                                    RandomResizedCrop, \n",
    "                                    Resize, \n",
    "                                    ToTensor)\n",
    "\n",
    "normalize = Normalize(mean=image_mean, std=image_std)\n",
    "_train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(size),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "_val_transforms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            CenterCrop(size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def train_transforms(examples):\n",
    "    examples['pixel_values'] = [_train_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples\n",
    "\n",
    "def val_transforms(examples):\n",
    "    examples['pixel_values'] = [_val_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the transforms\n",
    "train_ds.set_transform(train_transforms)\n",
    "val_ds.set_transform(val_transforms)\n",
    "test_ds.set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "train_batch_size = 8\n",
    "eval_batch_size = 8\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, shuffle=True, collate_fn=collate_fn, batch_size=train_batch_size)\n",
    "val_dataloader = DataLoader(val_ds, collate_fn=collate_fn, batch_size=eval_batch_size)\n",
    "test_dataloader = DataLoader(test_ds, collate_fn=collate_fn, batch_size=eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "  if isinstance(v, torch.Tensor):\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert batch['pixel_values'].shape == (train_batch_size, 3, 224, 224)\n",
    "assert batch['labels'].shape == (train_batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(val_dataloader))['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from transformers import ViTForImageClassification, AdamW\n",
    "import torch.nn as nn\n",
    "\n",
    "class ViTLightningModule(pl.LightningModule):\n",
    "    def __init__(self, num_labels=2):\n",
    "        super(ViTLightningModule, self).__init__()\n",
    "        self.vit = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k',\n",
    "                                                              num_labels=2,\n",
    "                                                              id2label=id2label,\n",
    "                                                              label2id=label2id)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        return outputs.logits\n",
    "        \n",
    "    def common_step(self, batch, batch_idx):\n",
    "        pixel_values = batch['pixel_values']\n",
    "        labels = batch['labels']\n",
    "        logits = self(pixel_values)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(logits, labels)\n",
    "        predictions = logits.argmax(-1)\n",
    "        correct = (predictions == labels).sum().item()\n",
    "        accuracy = correct/pixel_values.shape[0]\n",
    "\n",
    "        return loss, accuracy\n",
    "      \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self.common_step(batch, batch_idx)     \n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch\n",
    "        self.log(\"training_loss\", loss)\n",
    "        self.log(\"training_accuracy\", accuracy)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self.common_step(batch, batch_idx)     \n",
    "        self.log(\"validation_loss\", loss, on_epoch=True)\n",
    "        self.log(\"validation_accuracy\", accuracy, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self.common_step(batch, batch_idx)     \n",
    "        self.log(\"test_loss\", loss)\n",
    "        self.log(\"test_accuracy\", accuracy)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # We could make the optimizer more fancy by adding a scheduler and specifying which parameters do\n",
    "        # not require weight_decay but just using AdamW out-of-the-box works fine\n",
    "        return AdamW(self.parameters(), lr=5e-5)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return val_dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViTLightningModule.load_from_checkpoint(\"./lightning_logs/version_3/checkpoints/epoch=6-step=938.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image_path = '../data/test/mouth/Yawning/327781720_1287182322180121_2380712887574834957_n.jpg'\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "input_tensor = _val_transforms(image)\n",
    "input_batch = input_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda')\n",
    "input_batch = input_batch.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Perform prediction\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "\n",
    "# Get the predicted class or values based on your model's output format\n",
    "prediction = output.argmax()\n",
    "\n",
    "print(\"Predicted class:\", prediction.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0, 1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "video_path = '../data/transformer/val/yawning/10-MaleNoGlasses-Yawning.avi'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if the video opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "frames = []\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frames.append(frame)\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "#model = model.to('cuda')  \n",
    "\n",
    "predictions = []\n",
    "consecutive = 0\n",
    "\n",
    "for frame in frames:\n",
    "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    input_tensor = _val_transforms(image).to('cuda').unsqueeze(0)  \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "    prediction = output.argmax().item()\n",
    "    if prediction == 1:\n",
    "        consecutive += 1\n",
    "    else:\n",
    "        consecutive = 0\n",
    "    if consecutive == 60:\n",
    "        print(True)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Print or use the predictions as needed\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have 'predictions' and 'frames' from your previous code\n",
    "\n",
    "# Plotting\n",
    "plt.plot(range(1, len(predictions) + 1), predictions, marker='o')\n",
    "plt.xlabel('Frame Number')\n",
    "plt.ylabel('Yawn Prediction')\n",
    "plt.title('Predictions Over Video')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vidToFrame(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Check if the video opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        exit()\n",
    "\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(frames):\n",
    "    consecutive = 0\n",
    "\n",
    "    for frame in frames:\n",
    "        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        input_tensor = _val_transforms(image).to('cuda').unsqueeze(0)  \n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "        prediction = output.argmax().item()\n",
    "        if prediction == 1:\n",
    "            consecutive += 1\n",
    "        else:\n",
    "            consecutive = 0\n",
    "        if consecutive == 45:\n",
    "            return True\n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "TP = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "directory_path = '../data/transformer/combine/'\n",
    "wrong = []\n",
    "\n",
    "for folder_name in os.listdir(directory_path):\n",
    "    #if folder_name == \"talking\":\n",
    "    #    continue\n",
    "    folder_path = os.path.join(directory_path, folder_name)\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        frames = vidToFrame(file_path)\n",
    "        prediction = predict(frames)\n",
    "        if prediction and folder_name == \"yawning\":\n",
    "            correct += 1\n",
    "            TP += 1\n",
    "        elif prediction and (folder_name == \"normal\" or folder_name == \"talking\"):\n",
    "            FP += 1\n",
    "            wrong.append(file_path)\n",
    "        elif not prediction and (folder_name == \"normal\" or folder_name == \"talking\"):\n",
    "            correct += 1\n",
    "        elif not prediction and folder_name == \"yawning\":\n",
    "            FN += 1\n",
    "            wrong.append(file_path)\n",
    "        total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correct/total) # accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TP/(TP+FP)) # Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TP/(TP+FN)) # Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = '../data/transformer/combine/talking&yawning/'\n",
    "\n",
    "for file_name in os.listdir(directory_path):\n",
    "    file_path = os.path.join(directory_path, file_name)\n",
    "    frames = vidToFrame(file_path)\n",
    "    prediction = predict(frames)\n",
    "    if prediction:\n",
    "        correct += 1\n",
    "        TP += 1\n",
    "    else:\n",
    "        FN += 1\n",
    "        wrong.append(file_path)\n",
    "    total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(frames):\n",
    "    consecutive = 0\n",
    "    running = 0\n",
    "\n",
    "    for frame in frames:\n",
    "        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        input_tensor = _val_transforms(image).to('cuda').unsqueeze(0)  \n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "        prediction = output.argmax().item()\n",
    "        if prediction == 1:\n",
    "            running += 1\n",
    "        else:\n",
    "            consecutive = max(running, consecutive)\n",
    "            running = 0\n",
    "        \n",
    "    return max(running, consecutive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = '../data/transformer/combine/'\n",
    "columns = ['filename', 'frames']\n",
    "data_list = []\n",
    "\n",
    "for folder_name in os.listdir(directory_path):\n",
    "    if folder_name == \"yawning\":\n",
    "        continue\n",
    "    folder_path = os.path.join(directory_path, folder_name)\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        frames = vidToFrame(file_path)\n",
    "        prediction = predict(frames)\n",
    "        new_row = {'filename': file_path, 'frames': prediction}\n",
    "        data_list.append(new_row)\n",
    "\n",
    "df = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"noyawnFrames.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yawn = pd.read_csv(\"yawnFrames.csv\")\n",
    "df_Noyawn = pd.read_csv(\"noyawnFrames.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yawn = df_yawn[\"frames\"].values\n",
    "noyawn = df_Noyawn[\"frames\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_sizes = [30, 45, 60, 75]\n",
    "#threshold_sizes = np.arange(25, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold = None\n",
    "best_accuracy = 0\n",
    "precision = 0\n",
    "recall = 0\n",
    "\n",
    "for threshold in threshold_sizes:\n",
    "    yawn_classified = [size >= threshold for size in yawn]\n",
    "    noyawn_classified = [size < threshold for size in noyawn]\n",
    "\n",
    "    # Calculate accuracy\n",
    "    correct_yawn = sum(yawn_classified)\n",
    "    correct_noyawn = sum(noyawn_classified)\n",
    "    total_correct = correct_yawn + correct_noyawn\n",
    "    total_instances = len(yawn) + len(noyawn)\n",
    "    accuracy = total_correct / total_instances\n",
    "\n",
    "    # Update best threshold if accuracy improves\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_threshold = threshold\n",
    "        precision = correct_yawn / (correct_yawn + (len(noyawn) - correct_noyawn))\n",
    "        recall = correct_yawn / (correct_yawn + (len(yawn) - correct_yawn))\n",
    "\n",
    "print(f\"Best Threshold: {best_threshold}, Best Accuracy: {best_accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
