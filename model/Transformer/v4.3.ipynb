{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import imageio\n",
    "import medmnist\n",
    "import ipywidgets\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 128\n",
    "MAX_SEQ_LENGTH = 724\n",
    "frame_step = 2\n",
    "\n",
    "# DATA\n",
    "BATCH_SIZE = 8\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (MAX_SEQ_LENGTH, IMG_SIZE, IMG_SIZE, 3)\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 25\n",
    "\n",
    "# TUBELET EMBEDDING\n",
    "PATCH_SIZE = (200, 32, 32)\n",
    "#NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "# ViViT ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "PROJECTION_DIM = 128\n",
    "NUM_HEADS = 10\n",
    "NUM_LAYERS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/mirror-data.csv\")\n",
    "df[\"Action\"] = df[\"Action\"].str.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.Action != \"Talking&Yawning\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"] = df.Action.astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "dfTrain = pd.DataFrame()\n",
    "dfTest = pd.DataFrame()\n",
    "\n",
    "while i<len(df):\n",
    "    if i%5==0:\n",
    "        dfTest = pd.concat([dfTest, df.iloc[[i]]])\n",
    "    else :\n",
    "        dfTrain = pd.concat([dfTrain, df.iloc[[i]]])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Action</th>\n",
       "      <th>video-name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal</td>\n",
       "      <td>1-FemaleNoGlasses-Normal.avi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Talking</td>\n",
       "      <td>1-FemaleNoGlasses-Talking.avi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yawning</td>\n",
       "      <td>1-FemaleNoGlasses-Yawning.avi</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Normal</td>\n",
       "      <td>2-FemaleNoGlasses-Normal.avi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Talking</td>\n",
       "      <td>2-FemaleNoGlasses-Talking.avi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Action                     video-name  label\n",
       "0   Normal   1-FemaleNoGlasses-Normal.avi      0\n",
       "1  Talking  1-FemaleNoGlasses-Talking.avi      1\n",
       "2  Yawning  1-FemaleNoGlasses-Yawning.avi      2\n",
       "3   Normal   2-FemaleNoGlasses-Normal.avi      0\n",
       "4  Talking  2-FemaleNoGlasses-Talking.avi      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "print(dfTrain.shape[0])\n",
    "print(dfTest.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n",
      "80\n",
      "83\n"
     ]
    }
   ],
   "source": [
    "print(dfTrain[dfTrain.label == 0].shape[0])\n",
    "print(dfTrain[dfTrain.label == 1].shape[0])\n",
    "print(dfTrain[dfTrain.label == 2].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_frames(frame, output_size):\n",
    "  \"\"\"\n",
    "    Pad and resize an image from a video.\n",
    "\n",
    "    Args:\n",
    "      frame: Image that needs to resized and padded. \n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      Formatted frame with padding of specified output size.\n",
    "  \"\"\"\n",
    "  frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
    "  frame = tf.image.resize_with_pad(frame, *output_size)\n",
    "  return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_from_video_file(video_path, MAX_SEQ_LENGTH, frame_step, output_size = (IMG_SIZE, IMG_SIZE)):\n",
    "  \"\"\"\n",
    "    Creates frames from each video file present for each category.\n",
    "\n",
    "    Args:\n",
    "      video_path: File path to the video.\n",
    "      n_frames: Number of frames to be created per video file.\n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
    "  \"\"\"\n",
    "  # Read each video frame by frame\n",
    "  result = []\n",
    "  \n",
    "  src = cv2.VideoCapture(str(video_path))  \n",
    "\n",
    "  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "  need_length = 1 + (MAX_SEQ_LENGTH - 1) * frame_step\n",
    "\n",
    "  if need_length > video_length:\n",
    "    start = 0\n",
    "  else:\n",
    "    max_start = video_length - need_length\n",
    "    start = random.randint(0, max_start + 1)\n",
    "\n",
    "  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "  # ret is a boolean indicating whether read was successful, frame is the image itself\n",
    "  ret, frame = src.read()\n",
    "  result.append(format_frames(frame, output_size))\n",
    "\n",
    "  for _ in range(MAX_SEQ_LENGTH - 1):\n",
    "    for _ in range(frame_step):\n",
    "      ret, frame = src.read()\n",
    "    if ret:\n",
    "      frame = format_frames(frame, output_size)\n",
    "      result.append(frame)\n",
    "    else:\n",
    "      result.append(np.zeros_like(result[0]))\n",
    "  src.release()\n",
    "  result = np.array(result)[..., [2, 1, 0]]\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameGenerator:\n",
    "  def __init__(self, df, root_dir, MAX_SEQ_LENGTH, frame_step):\n",
    "    \"\"\" Returns a set of frames with their associated label. \n",
    "\n",
    "      Args:\n",
    "        path: Video file paths.\n",
    "        n_frames: Number of frames. \n",
    "        training: Boolean to determine if training dataset is being created.\n",
    "    \"\"\"\n",
    "    self.df = df\n",
    "    self.n_frames = MAX_SEQ_LENGTH\n",
    "    self.root_dir = root_dir\n",
    "    self.frame_step = frame_step\n",
    "\n",
    "  def __call__(self):\n",
    "    video_paths = self.df[\"video-name\"].values.tolist()\n",
    "    labels = self.df[\"label\"].values.tolist()\n",
    "\n",
    "    for idx, path in enumerate(video_paths):\n",
    "      video_frames = frames_from_video_file(os.path.join(self.root_dir, path), self.n_frames, frame_step) \n",
    "      yield video_frames, labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n",
    "                    tf.TensorSpec(shape = (), dtype = tf.int8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_generator(FrameGenerator(dfTrain, \"../data/YawDD/YawDD dataset/Mirror/all/\", MAX_SEQ_LENGTH, frame_step),  output_signature = output_signature)\n",
    "test_ds = tf.data.Dataset.from_generator(FrameGenerator(dfTest, \"../data/YawDD/YawDD dataset/Mirror/all/\", MAX_SEQ_LENGTH, frame_step),  output_signature = output_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)\n",
    "test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.batch(BATCH_SIZE)\n",
    "test_ds = test_ds.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TubeletEmbedding(layers.Layer):\n",
    "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.projection = layers.Conv3D(\n",
    "            filters=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=patch_size,\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
    "\n",
    "    def call(self, videos):\n",
    "        projected_patches = self.projection(videos)\n",
    "        flattened_patches = self.flatten(projected_patches)\n",
    "        return flattened_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, num_tokens, _ = input_shape\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_tokens, output_dim=self.embed_dim\n",
    "        )\n",
    "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
    "\n",
    "    def call(self, encoded_tokens):\n",
    "        # Encode the positions and add it to the encoded tokens\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_tokens = encoded_tokens + encoded_positions\n",
    "        return encoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vivit_classifier(\n",
    "    tubelet_embedder,\n",
    "    positional_encoder,\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    transformer_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embed_dim=PROJECTION_DIM,\n",
    "    layer_norm_eps=LAYER_NORM_EPS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "):\n",
    "    # Get the input layer\n",
    "    inputs = layers.Input(shape=(input_shape))\n",
    "    # Create patches.\n",
    "    patches = tubelet_embedder(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = positional_encoder(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization and MHSA\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=embed_dim * 4, activation=tf.nn.gelu),\n",
    "                layers.Dense(units=embed_dim, activation=tf.nn.gelu),\n",
    "            ]\n",
    "        )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
    "    representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(units=num_classes, activation=\"softmax\")(representation)\n",
    "\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "    # Initialize model\n",
    "    model = create_vivit_classifier(\n",
    "        tubelet_embedder=TubeletEmbedding(\n",
    "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
    "        ),\n",
    "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
    "    )\n",
    "\n",
    "    # Compile the model with the optimizer, loss function\n",
    "    # and the metrics.\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Train the model.\n",
    "    _ = model.fit(train_ds, epochs=EPOCHS, validation_data = test_ds)\n",
    "\n",
    "    _, accuracy = model.evaluate(test_ds)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    #print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "31/31 [==============================] - 472s 4s/step - loss: 1.1392 - accuracy: 0.4385 - val_loss: 1.1694 - val_accuracy: 0.4516\n",
      "Epoch 2/25\n",
      "31/31 [==============================] - 20s 644ms/step - loss: 0.8989 - accuracy: 0.5615 - val_loss: 1.1111 - val_accuracy: 0.5000\n",
      "Epoch 3/25\n",
      "31/31 [==============================] - 18s 583ms/step - loss: 0.8183 - accuracy: 0.5984 - val_loss: 1.1316 - val_accuracy: 0.4839\n",
      "Epoch 4/25\n",
      "31/31 [==============================] - 18s 578ms/step - loss: 0.7910 - accuracy: 0.5943 - val_loss: 1.1062 - val_accuracy: 0.4516\n",
      "Epoch 5/25\n",
      "31/31 [==============================] - 18s 581ms/step - loss: 0.7882 - accuracy: 0.5902 - val_loss: 1.1506 - val_accuracy: 0.4516\n",
      "Epoch 6/25\n",
      "31/31 [==============================] - 18s 575ms/step - loss: 0.7572 - accuracy: 0.6107 - val_loss: 1.0931 - val_accuracy: 0.5484\n",
      "Epoch 7/25\n",
      "31/31 [==============================] - 18s 577ms/step - loss: 0.7719 - accuracy: 0.5779 - val_loss: 1.0906 - val_accuracy: 0.5645\n",
      "Epoch 8/25\n",
      "31/31 [==============================] - 18s 579ms/step - loss: 0.7514 - accuracy: 0.6434 - val_loss: 1.1326 - val_accuracy: 0.4516\n",
      "Epoch 9/25\n",
      "31/31 [==============================] - 18s 578ms/step - loss: 0.7605 - accuracy: 0.6148 - val_loss: 1.1453 - val_accuracy: 0.4677\n",
      "Epoch 10/25\n",
      "31/31 [==============================] - 18s 583ms/step - loss: 0.7466 - accuracy: 0.6311 - val_loss: 1.1544 - val_accuracy: 0.5000\n",
      "Epoch 11/25\n",
      "31/31 [==============================] - 18s 583ms/step - loss: 0.7442 - accuracy: 0.6393 - val_loss: 1.1565 - val_accuracy: 0.5000\n",
      "Epoch 12/25\n",
      "31/31 [==============================] - 18s 579ms/step - loss: 0.7536 - accuracy: 0.6393 - val_loss: 1.1512 - val_accuracy: 0.5484\n",
      "Epoch 13/25\n",
      "31/31 [==============================] - 18s 577ms/step - loss: 0.7314 - accuracy: 0.6311 - val_loss: 1.2185 - val_accuracy: 0.5161\n",
      "Epoch 14/25\n",
      "31/31 [==============================] - 18s 578ms/step - loss: 0.7330 - accuracy: 0.6270 - val_loss: 1.2047 - val_accuracy: 0.5484\n",
      "Epoch 15/25\n",
      "31/31 [==============================] - 18s 576ms/step - loss: 0.7273 - accuracy: 0.6434 - val_loss: 1.2226 - val_accuracy: 0.5484\n",
      "Epoch 16/25\n",
      "31/31 [==============================] - 18s 577ms/step - loss: 0.7383 - accuracy: 0.6189 - val_loss: 1.2719 - val_accuracy: 0.5000\n",
      "Epoch 17/25\n",
      "31/31 [==============================] - 18s 577ms/step - loss: 0.7151 - accuracy: 0.6762 - val_loss: 1.2551 - val_accuracy: 0.5484\n",
      "Epoch 18/25\n",
      "31/31 [==============================] - 18s 577ms/step - loss: 0.7144 - accuracy: 0.6557 - val_loss: 1.4110 - val_accuracy: 0.4355\n",
      "Epoch 19/25\n",
      "31/31 [==============================] - 18s 576ms/step - loss: 0.7242 - accuracy: 0.6516 - val_loss: 1.3194 - val_accuracy: 0.5000\n",
      "Epoch 20/25\n",
      "31/31 [==============================] - 18s 577ms/step - loss: 0.7112 - accuracy: 0.6434 - val_loss: 1.3481 - val_accuracy: 0.4677\n",
      "Epoch 21/25\n",
      "31/31 [==============================] - 18s 578ms/step - loss: 0.7067 - accuracy: 0.6803 - val_loss: 1.3601 - val_accuracy: 0.4677\n",
      "Epoch 22/25\n",
      "31/31 [==============================] - 18s 583ms/step - loss: 0.7179 - accuracy: 0.6352 - val_loss: 1.3394 - val_accuracy: 0.5323\n",
      "Epoch 23/25\n",
      "31/31 [==============================] - 18s 579ms/step - loss: 0.7039 - accuracy: 0.6721 - val_loss: 1.3792 - val_accuracy: 0.5161\n",
      "Epoch 24/25\n",
      "31/31 [==============================] - 18s 581ms/step - loss: 0.7130 - accuracy: 0.6516 - val_loss: 1.4566 - val_accuracy: 0.4032\n",
      "Epoch 25/25\n",
      "31/31 [==============================] - 18s 576ms/step - loss: 0.6977 - accuracy: 0.6680 - val_loss: 1.3703 - val_accuracy: 0.5484\n",
      "8/8 [==============================] - 3s 424ms/step - loss: 1.3703 - accuracy: 0.5484\n",
      "Test accuracy: 54.84%\n"
     ]
    }
   ],
   "source": [
    "trained_model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2be4fae62b40ff8f1b9940933aaa74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(VBox(children=(HTML(value=\"'T: 0 | P: 2'\"), Box(children=(Image(value=b'GIF89a\\x80\\x00\\x80\\xâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_SAMPLES_VIZ = 4\n",
    "t = iter(test_ds)\n",
    "for i in range(1):\n",
    "    testsamples, labels = t.get_next()\n",
    "testsamples, labels = testsamples[:NUM_SAMPLES_VIZ], labels[:NUM_SAMPLES_VIZ]\n",
    "\n",
    "ground_truths = []\n",
    "preds = []\n",
    "videos = []\n",
    "\n",
    "for i, (testsample, label) in enumerate(zip(testsamples, labels)):\n",
    "    # Generate gif\n",
    "    with io.BytesIO() as gif:\n",
    "        imageio.mimsave(gif, (testsample.numpy()*255).astype(\"uint8\"), \"GIF\")\n",
    "        videos.append(gif.getvalue())\n",
    "\n",
    "    # Get model prediction\n",
    "    output = trained_model.predict(tf.expand_dims(testsample, axis=0))[0]\n",
    "    pred = np.argmax(output, axis=0)\n",
    "\n",
    "    ground_truths.append(label.numpy().astype(\"int\"))\n",
    "    preds.append(pred)\n",
    "\n",
    "\n",
    "def make_box_for_grid(image_widget, fit):\n",
    "    \"\"\"Make a VBox to hold caption/image for demonstrating option_fit values.\n",
    "\n",
    "    Source: https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20Styling.html\n",
    "    \"\"\"\n",
    "    # Make the caption\n",
    "    if fit is not None:\n",
    "        fit_str = \"'{}'\".format(fit)\n",
    "    else:\n",
    "        fit_str = str(fit)\n",
    "\n",
    "    h = ipywidgets.HTML(value=\"\" + str(fit_str) + \"\")\n",
    "\n",
    "    # Make the green box with the image widget inside it\n",
    "    boxb = ipywidgets.widgets.Box()\n",
    "    boxb.children = [image_widget]\n",
    "\n",
    "    # Compose into a vertical box\n",
    "    vb = ipywidgets.widgets.VBox()\n",
    "    vb.layout.align_items = \"center\"\n",
    "    vb.children = [h, boxb]\n",
    "    return vb\n",
    "\n",
    "\n",
    "boxes = []\n",
    "class_vocab = [\"Normal\", \"Talking\", \"Yawning\"]\n",
    "\n",
    "for i in range(NUM_SAMPLES_VIZ):\n",
    "    ib = ipywidgets.widgets.Image(value=videos[i], width=100, height=100)\n",
    "    true_class = str(ground_truths[i])\n",
    "    pred_class = str(preds[i])\n",
    "    caption = f\"T: {true_class} | P: {pred_class}\"\n",
    "\n",
    "    boxes.append(make_box_for_grid(ib, caption))\n",
    "\n",
    "ipywidgets.widgets.GridBox(\n",
    "    boxes, layout=ipywidgets.widgets.Layout(grid_template_columns=\"repeat(5, 200px)\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
